{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a02e511-aecb-4ab4-bb48-5941e86277e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from sklearn.metrics.pairwise import cosine_distances as cd\n",
    "from collections import defaultdict\n",
    "from pyeer.eer_info import get_eer_stats\n",
    "from sklearn.metrics.pairwise import manhattan_distances as md\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "\n",
    "from pytorch_metric_learning.distances import LpDistance, CosineSimilarity,SNRDistance\n",
    "\n",
    "def EERf(resutls):\n",
    "    resutls= np.array(resutls)\n",
    "    genuine  = resutls[resutls[:, 1] == 1][:, 0]\n",
    "    impostor = resutls[resutls[:, 1] == 0][:, 0]\n",
    "    stats_a = get_eer_stats(genuine, impostor)\n",
    "    return(stats_a.eer,stats_a.fmr100)\n",
    "\n",
    "def calculate_and_print_averages(y_train, tn, resutls3):\n",
    "    u, counts = np.unique(y_train, return_counts=True)\n",
    "    eer_values = []\n",
    "    ii = 0\n",
    "\n",
    "    for i in resutls3.keys():\n",
    "        re = EERf(resutls3[i])\n",
    "        eer = re[0]\n",
    "        print(f\"{i}: EER = {re[0]:.4f}, FMR100 = {re[1]:.4f}, Count = {counts[ii]}\")\n",
    "        eer_values.append(eer)\n",
    "        ii += 1\n",
    "\n",
    "    average_eer = np.mean(eer_values) * 100\n",
    "    std_eer = np.std(eer_values) * 100\n",
    "    \n",
    "    print(f\"Final Average EER: {average_eer:.4f}\")\n",
    "    print(f\"Final EER Standard Deviation: {std_eer:.4f}\")\n",
    "    print(f\"${average_eer:.2f} \\\\pm {std_eer:.2f}$\")\n",
    "    return(average_eer,std_eer)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_similarity_scores_two(enrollment_embeddings, y_enrollment, verification_embeddings, y_verification,distance):\n",
    "    similarity_results = []\n",
    "    similarity_results_by_class = []\n",
    "    similarity_results_by_class_dict = defaultdict(list)\n",
    "    unique_classes = np.unique(y_enrollment)\n",
    "    class_indices = [np.where(y_enrollment == cls)[0] for cls in unique_classes]\n",
    "\n",
    "    if distance == \"cd\":\n",
    "        similarity_matrix = -1 * cd(verification_embeddings, enrollment_embeddings)\n",
    "    elif distance == \"ed\":\n",
    "        similarity_matrix = -1 * ed(verification_embeddings, enrollment_embeddings)\n",
    "\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        current_class = y_verification[i]\n",
    "        predicted_scores = similarity_matrix[i]\n",
    "        same_class_indices = class_indices[np.where(unique_classes == current_class)[0][0]]\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            same_class_indices = class_indices[np.where(unique_classes == cls)[0][0]]\n",
    "            max_score = sum(sorted(predicted_scores[same_class_indices], reverse=True)[:10]) / 10\n",
    "            if current_class == cls:\n",
    "                similarity_results_by_class.append([max_score, 1, current_class, cls, i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 1, current_class, cls, i, cls])\n",
    "            else:\n",
    "                similarity_results_by_class.append([max_score, 0, current_class, cls, i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 0, current_class, cls, i, cls])\n",
    "\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict\n",
    "\n",
    "def assessment_model_data_two(enrollment_data, ye, verification_data, yv, e_network, distance):\n",
    "    x_enrollment, y_enrollment = enrollment_data, ye\n",
    "    x_verification, y_verification = verification_data, yv\n",
    "    enrollment_embeddings = compute_embedding_batch_two(x_enrollment, e_network)\n",
    "    verification_embeddings = compute_embedding_batch_two(x_verification, e_network)\n",
    "    similarity_results_by_class, similarity_results_by_class_dict = calculate_similarity_scores_two(\n",
    "        enrollment_embeddings, y_enrollment, verification_embeddings, y_verification,distance\n",
    "    )\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_embedding_batch_two(x_test_batch, embedding_network, batch_size=150, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Computes embeddings for the input data in batches, including batch-wise tensor conversion.\n",
    "\n",
    "    Args:\n",
    "        x_test_batch (numpy.ndarray): Input data (in numpy format).\n",
    "        embedding_network (torch.nn.Module): The network to compute embeddings.\n",
    "        batch_size (int, optional): Size of each batch for processing. Default is 100.\n",
    "        device (str, optional): The device to use ('cuda' or 'cpu'). Default is 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Computed embeddings for the input data.\n",
    "    \"\"\"\n",
    "    total_samples = len(x_test_batch)\n",
    "    embeddings = []\n",
    "\n",
    "    # Process the data in batches\n",
    "    for start_index in range(0, total_samples, batch_size):\n",
    "        end_index = min(start_index + batch_size, total_samples)\n",
    "\n",
    "        # Convert the current batch to tensor and move to device\n",
    "        batch = torch.tensor(x_test_batch[start_index:end_index], dtype=torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            batch_embeddings = embedding_network(batch).detach().cpu().numpy()\n",
    "\n",
    "        embeddings.append(batch_embeddings)  # Store embeddings on CPU\n",
    "\n",
    "        # Clear GPU memory for the batch\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all embeddings into a single numpy array\n",
    "    anchor_embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "    print(f\"Computed embeddings shape: {anchor_embeddings.shape}\")\n",
    "    return anchor_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49900328-8133-4d21-aa6f-642369d7814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EEGNet7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet7, self).__init__()\n",
    "        # InstanceNorm1d normalizes each channel across its 500 values for each sample\n",
    "        self.norm = nn.InstanceNorm1d(93, affine=False)  # Normalizes across the 500 values in each of the 93 channels\n",
    "        self.act = nn.ReLU()  # or nn.SELU() , ReLU\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.conv1 = nn.Conv2d(1, 256, (1, 4), padding='same')\n",
    "        self.pool1 = nn.MaxPool2d((1, 2))\n",
    "        self.conv2 = nn.Conv2d(256, 192, (4, 1), padding='same')\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))\n",
    "        self.conv3 = nn.Conv2d(192, 128, (1, 4), padding='same')\n",
    "        self.pool3 = nn.MaxPool2d((1, 2))\n",
    "        self.conv4 = nn.Conv2d(128, 96, (4, 1), padding='same')\n",
    "        self.pool4 = nn.MaxPool2d((2, 1))\n",
    "        self.conv5 = nn.Conv2d(96, 64, (1, 4), padding='same')\n",
    "        self.pool5 = nn.MaxPool2d((1, 2))\n",
    "        self.conv6 = nn.Conv2d(64, 32, (4, 1), padding='same')\n",
    "        self.pool6 = nn.MaxPool2d((2, 1))\n",
    "        self.conv7 = nn.Conv2d(32, 16, (1, 2), padding='same')\n",
    "        self.conv8 = nn.Conv2d(16, 2, (2, 1), padding='same')\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.embedding = nn.Linear(1364, 128)  # Embedding layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        #if x.dim() == 3:\n",
    "            #x = x.unsqueeze(1)  # Adding a channel dimension if input is 3D\n",
    "        x = self.norm(x)  # Apply InstanceNorm along the channel dimension (squeeze to 1D first)\n",
    "        x = x.unsqueeze(1)  # Reshape to 4D again for Conv2d\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.act(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.act(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.act(self.conv5(x))\n",
    "        x = self.pool5(x)\n",
    "        x = self.act(self.conv6(x))\n",
    "        x = self.pool6(x)\n",
    "        x = self.act(self.conv7(x))\n",
    "        x = self.act(self.conv8(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.embedding(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trunk = EEGNet7().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df0d8efc-7612-421b-a3ed-999475096fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 70, Number of unique sessions: 19\n",
      "Subject 82, Number of unique sessions: 19\n",
      "Subject 85, Number of unique sessions: 19\n",
      "Subject 86, Number of unique sessions: 19\n",
      "Subject 96, Number of unique sessions: 18\n",
      "Subject 103, Number of unique sessions: 18\n",
      "Subject 106, Number of unique sessions: 41\n",
      "Subject 109, Number of unique sessions: 5\n",
      "Subject 111, Number of unique sessions: 19\n",
      "Subject 115, Number of unique sessions: 41\n",
      "Subject 118, Number of unique sessions: 19\n",
      "Subject 119, Number of unique sessions: 8\n",
      "Subject 125, Number of unique sessions: 19\n",
      "Subject 129, Number of unique sessions: 5\n",
      "Subject 130, Number of unique sessions: 19\n",
      "Subject 131, Number of unique sessions: 19\n",
      "Subject 136, Number of unique sessions: 19\n",
      "Subject 145, Number of unique sessions: 19\n",
      "Subject 156, Number of unique sessions: 6\n",
      "Subject 159, Number of unique sessions: 19\n",
      "Subject 167, Number of unique sessions: 2\n",
      "Subject 169, Number of unique sessions: 6\n",
      "Subject 173, Number of unique sessions: 6\n",
      "Subject 174, Number of unique sessions: 19\n",
      "Subject 175, Number of unique sessions: 6\n",
      "Subject 176, Number of unique sessions: 6\n",
      "Subject 181, Number of unique sessions: 5\n",
      "Subject 183, Number of unique sessions: 6\n",
      "Subject 184, Number of unique sessions: 19\n",
      "Subject 187, Number of unique sessions: 32\n",
      "Subject 191, Number of unique sessions: 19\n",
      "Subject 192, Number of unique sessions: 19\n",
      "Subject 194, Number of unique sessions: 19\n",
      "Subject 197, Number of unique sessions: 19\n",
      "Subject 199, Number of unique sessions: 19\n",
      "Subject 201, Number of unique sessions: 19\n",
      "Subject 203, Number of unique sessions: 6\n",
      "Subject 206, Number of unique sessions: 6\n",
      "Subject 207, Number of unique sessions: 41\n",
      "Subject 214, Number of unique sessions: 18\n",
      "Subject 216, Number of unique sessions: 6\n",
      "Subject 217, Number of unique sessions: 6\n",
      "Subject 219, Number of unique sessions: 5\n",
      "Subject 221, Number of unique sessions: 5\n",
      "Subject 225, Number of unique sessions: 6\n",
      "Subject 230, Number of unique sessions: 6\n",
      "Subject 231, Number of unique sessions: 19\n",
      "Subject 233, Number of unique sessions: 19\n",
      "Subject 236, Number of unique sessions: 42\n",
      "Subject 245, Number of unique sessions: 6\n",
      "Subject 249, Number of unique sessions: 42\n",
      "Subject 250, Number of unique sessions: 41\n",
      "Subject 251, Number of unique sessions: 41\n",
      "Subject 257, Number of unique sessions: 6\n",
      "Subject 260, Number of unique sessions: 49\n",
      "Subject 261, Number of unique sessions: 10\n",
      "Subject 262, Number of unique sessions: 6\n",
      "Subject 265, Number of unique sessions: 40\n",
      "Subject 268, Number of unique sessions: 19\n",
      "Subject 270, Number of unique sessions: 19\n",
      "Subject 277, Number of unique sessions: 13\n",
      "Subject 281, Number of unique sessions: 19\n",
      "Subject 282, Number of unique sessions: 5\n",
      "Subject 284, Number of unique sessions: 19\n",
      "Subject 286, Number of unique sessions: 19\n",
      "Subject 291, Number of unique sessions: 19\n",
      "Subject 296, Number of unique sessions: 23\n",
      "Subject 299, Number of unique sessions: 22\n",
      "Subject 301, Number of unique sessions: 23\n",
      "Subject 303, Number of unique sessions: 21\n",
      "Subject 306, Number of unique sessions: 23\n",
      "Subject 310, Number of unique sessions: 23\n",
      "Subject 314, Number of unique sessions: 6\n",
      "Subject 322, Number of unique sessions: 21\n",
      "Subject 325, Number of unique sessions: 22\n",
      "Subject 332, Number of unique sessions: 4\n",
      "Subject 336, Number of unique sessions: 23\n",
      "Subject 342, Number of unique sessions: 23\n",
      "Subject 348, Number of unique sessions: 23\n",
      "Subject 354, Number of unique sessions: 23\n",
      "Subject 357, Number of unique sessions: 22\n",
      "Subject 364, Number of unique sessions: 22\n",
      "Subject 365, Number of unique sessions: 30\n",
      "Subject 373, Number of unique sessions: 23\n",
      "Subject 374, Number of unique sessions: 20\n",
      "Subject 384, Number of unique sessions: 5\n",
      "Subject 385, Number of unique sessions: 22\n",
      "Subject 387, Number of unique sessions: 23\n",
      "Subject 402, Number of unique sessions: 9\n",
      "Subject 404, Number of unique sessions: 8\n",
      "Subject 407, Number of unique sessions: 9\n",
      "Subject 413, Number of unique sessions: 9\n",
      "Subject 414, Number of unique sessions: 9\n",
      "Subject 415, Number of unique sessions: 9\n",
      "Subject 418, Number of unique sessions: 9\n",
      "Subject 419, Number of unique sessions: 9\n",
      "Subject 422, Number of unique sessions: 8\n",
      "Subject 423, Number of unique sessions: 9\n",
      "Subject 433, Number of unique sessions: 7\n",
      "Subject 436, Number of unique sessions: 8\n",
      "x_test_e: (10000, 93, 500), y_test_e: (10000,), s_test_e: (10000,)\n",
      "x_test_v: (160051, 93, 500), y_test_v: (160051,), s_test_v: (160051,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Load test data from test_raw.h5\n",
    "with h5py.File(\"./data/test_raw.h5\", \"r\") as f_test:\n",
    "    X_test = f_test['data'][:]\n",
    "    Y_test = f_test['labels'][:]\n",
    "    S_test = f_test['sessions'][:]\n",
    "    H_test = f_test['hardwares'][:]  # Load the 'hardwares' dataset as h_test\n",
    "\n",
    "# Load negative data from neg_raw.h5\n",
    "with h5py.File(\"./data/neg_raw.h5\", \"r\") as f_neg:\n",
    "    X_neg = f_neg['data'][:]\n",
    "    Y_neg = f_neg['labels'][:]\n",
    "    S_neg = f_neg['sessions'][:]\n",
    "    H_neg = f_neg['hardwares'][:]  # Load the 'hardwares' dataset as h_neg\n",
    "\n",
    "# Optional: Combine or use them independently depending on your requirement\n",
    "# Example of concatenating them for a combined dataset\n",
    "X_test = np.concatenate((X_test, X_neg), axis=0)\n",
    "Y_test = np.concatenate((Y_test, Y_neg), axis=0)\n",
    "S_test = np.concatenate((S_test, S_neg), axis=0)\n",
    "H_test = np.concatenate((H_test, H_neg), axis=0)\n",
    "\n",
    "# Now, X_combined, Y_combined, S_combined, and h_combined contain the merged data\n",
    "\n",
    "\n",
    "# Now, X_combined, Y_combined, and S_combined contain the merged data\n",
    "\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Reorder the arrays according to the shuffled indices\n",
    "x_test = X_test[indices]\n",
    "y_test = Y_test[indices]\n",
    "s_test = S_test[indices]\n",
    "h_test = H_test[indices]\n",
    "\n",
    "\n",
    "# Find unique subjects\n",
    "unique_subjects = np.unique(y_test)\n",
    "\n",
    "# Initialize lists to hold the data for x_test_e and x_test_v\n",
    "x_test_e_list = []\n",
    "y_test_e_list = []\n",
    "s_test_e_list = []\n",
    "h_test_e_list = []\n",
    "\n",
    "x_test_v_list = []\n",
    "y_test_v_list = []\n",
    "s_test_v_list = []\n",
    "h_test_v_list = []\n",
    "\n",
    "# Assign the minimum session for each subject to x_test_e and the rest to x_test_v\n",
    "for subject in unique_subjects:\n",
    "    subject_indices = np.where(y_test == subject)[0]\n",
    "    subject_sessions = s_test[subject_indices]\n",
    "    \n",
    "    # Skip subjects with fewer than two unique sessions\n",
    "    if len(np.unique(subject_sessions)) < 2:\n",
    "        continue\n",
    "\n",
    "    print(f\"Subject {subject}, Number of unique sessions: {len(np.unique(subject_sessions))}\")\n",
    "    \n",
    "    # Assign the minimum session to the evaluation set (x_test_e)\n",
    "    min_session = np.min(subject_sessions)\n",
    "    \n",
    "    # Append data to the evaluation set (min session)\n",
    "    x_test_e_list.extend(x_test[subject_indices][subject_sessions == min_session])\n",
    "    y_test_e_list.extend(y_test[subject_indices][subject_sessions == min_session])\n",
    "    s_test_e_list.extend(s_test[subject_indices][subject_sessions == min_session])\n",
    "    h_test_e_list.extend(h_test[subject_indices][subject_sessions == min_session])\n",
    "\n",
    "    # Append remaining sessions to the validation set (x_test_v)\n",
    "    x_test_v_list.extend(x_test[subject_indices][subject_sessions != min_session])\n",
    "    y_test_v_list.extend(y_test[subject_indices][subject_sessions != min_session])\n",
    "    s_test_v_list.extend(s_test[subject_indices][subject_sessions != min_session])\n",
    "    h_test_v_list.extend(h_test[subject_indices][subject_sessions != min_session])\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_e\n",
    "indices_e = np.arange(len(x_test_e_list))\n",
    "np.random.shuffle(indices_e)\n",
    "\n",
    "x_test_e = np.array(x_test_e_list)[indices_e]\n",
    "y_test_e = np.array(y_test_e_list)[indices_e]\n",
    "s_test_e = np.array(s_test_e_list)[indices_e]\n",
    "h_test_e = np.array(h_test_v_list)[indices_e]\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_v\n",
    "indices_v = np.arange(len(x_test_v_list))\n",
    "np.random.shuffle(indices_v)\n",
    "\n",
    "x_test_v = np.array(x_test_v_list)[indices_v]\n",
    "y_test_v = np.array(y_test_v_list)[indices_v]\n",
    "s_test_v = np.array(s_test_v_list)[indices_v]\n",
    "h_test_v = np.array(h_test_v_list)[indices_v]\n",
    "# Optional: Save the new test evaluation and validation sets to npy files (if needed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"x_test_e: {x_test_e.shape}, y_test_e: {y_test_e.shape}, s_test_e: {s_test_e.shape}\")\n",
    "print(f\"x_test_v: {x_test_v.shape}, y_test_v: {y_test_v.shape}, s_test_v: {s_test_v.shape}\")\n",
    "\n",
    "\n",
    "# Load the state dictionary from the saved file\n",
    "state_dict = torch.load('./model/SupConLoss128_m4_e99_HydroCe_unconnected.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "trunk.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c857d69b-1c5d-4085-9bd6-2807d0841159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings shape: (10000, 128)\n",
      "Computed embeddings shape: (160051, 128)\n",
      "70: EER = 0.3595, FMR100 = 0.8972, Count = 100\n",
      "82: EER = 0.0439, FMR100 = 0.1306, Count = 100\n",
      "85: EER = 0.1607, FMR100 = 0.6189, Count = 100\n",
      "86: EER = 0.1700, FMR100 = 0.3417, Count = 100\n",
      "96: EER = 0.1983, FMR100 = 0.6300, Count = 100\n",
      "103: EER = 0.3418, FMR100 = 0.7806, Count = 100\n",
      "106: EER = 0.2891, FMR100 = 0.7393, Count = 100\n",
      "109: EER = 0.2325, FMR100 = 0.5200, Count = 100\n",
      "111: EER = 0.1978, FMR100 = 0.6711, Count = 100\n",
      "115: EER = 0.3181, FMR100 = 0.7783, Count = 100\n",
      "118: EER = 0.3039, FMR100 = 0.6017, Count = 100\n",
      "119: EER = 0.5129, FMR100 = 0.9757, Count = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fallahi/anaconda3/envs/oml/lib/python3.9/site-packages/pyeer/eer_stats.py:219: UserWarning: It is possible that you had set the wrong score type. Please consider reviewing if you are using dissimilarity or similarity scores\n",
      "  warn(\"It is possible that you had set the wrong score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125: EER = 0.5861, FMR100 = 0.7783, Count = 100\n",
      "129: EER = 0.4750, FMR100 = 0.9925, Count = 100\n",
      "130: EER = 0.4350, FMR100 = 0.6344, Count = 100\n",
      "131: EER = 0.4945, FMR100 = 0.9067, Count = 100\n",
      "136: EER = 0.6117, FMR100 = 0.9544, Count = 100\n",
      "145: EER = 0.3250, FMR100 = 0.6339, Count = 100\n",
      "156: EER = 0.0720, FMR100 = 0.1960, Count = 100\n",
      "159: EER = 0.3372, FMR100 = 0.6333, Count = 100\n",
      "167: EER = 0.2100, FMR100 = 0.7700, Count = 100\n",
      "169: EER = 0.0900, FMR100 = 0.3140, Count = 100\n",
      "173: EER = 0.1280, FMR100 = 0.5840, Count = 100\n",
      "174: EER = 0.5617, FMR100 = 0.9761, Count = 100\n",
      "175: EER = 0.1007, FMR100 = 0.3260, Count = 100\n",
      "176: EER = 0.2083, FMR100 = 0.4120, Count = 100\n",
      "181: EER = 0.1532, FMR100 = 0.6550, Count = 100\n",
      "183: EER = 0.1660, FMR100 = 0.4680, Count = 100\n",
      "184: EER = 0.4778, FMR100 = 0.9650, Count = 100\n",
      "187: EER = 0.7229, FMR100 = 0.9890, Count = 100\n",
      "191: EER = 0.5267, FMR100 = 0.8778, Count = 100\n",
      "192: EER = 0.3928, FMR100 = 0.9789, Count = 100\n",
      "194: EER = 0.1723, FMR100 = 0.7056, Count = 100\n",
      "197: EER = 0.3167, FMR100 = 0.7283, Count = 100\n",
      "199: EER = 0.3261, FMR100 = 0.5622, Count = 100\n",
      "201: EER = 0.5039, FMR100 = 0.7256, Count = 100\n",
      "203: EER = 0.0181, FMR100 = 0.0300, Count = 100\n",
      "206: EER = 0.0384, FMR100 = 0.2280, Count = 100\n",
      "207: EER = 0.6600, FMR100 = 0.9150, Count = 100\n",
      "214: EER = 0.3659, FMR100 = 0.7278, Count = 100\n",
      "216: EER = 0.3540, FMR100 = 0.4500, Count = 100\n",
      "217: EER = 0.7884, FMR100 = 1.0000, Count = 100\n",
      "219: EER = 0.1862, FMR100 = 0.3491, Count = 100\n",
      "221: EER = 0.0911, FMR100 = 0.5250, Count = 100\n",
      "225: EER = 0.3480, FMR100 = 0.6420, Count = 100\n",
      "230: EER = 0.3221, FMR100 = 0.8740, Count = 100\n",
      "231: EER = 0.2878, FMR100 = 0.6967, Count = 100\n",
      "233: EER = 0.5783, FMR100 = 0.8867, Count = 100\n",
      "236: EER = 0.5351, FMR100 = 0.9788, Count = 100\n",
      "245: EER = 0.0761, FMR100 = 0.3900, Count = 100\n",
      "249: EER = 0.3691, FMR100 = 0.8812, Count = 100\n",
      "250: EER = 0.3332, FMR100 = 0.9137, Count = 100\n",
      "251: EER = 0.4108, FMR100 = 0.8815, Count = 100\n",
      "257: EER = 0.1840, FMR100 = 0.5740, Count = 100\n",
      "260: EER = 0.1748, FMR100 = 0.5400, Count = 100\n",
      "261: EER = 0.3505, FMR100 = 0.8233, Count = 100\n",
      "262: EER = 0.0100, FMR100 = 0.0100, Count = 100\n",
      "265: EER = 0.3234, FMR100 = 0.9364, Count = 100\n",
      "268: EER = 0.3356, FMR100 = 0.7717, Count = 100\n",
      "270: EER = 0.3278, FMR100 = 0.9378, Count = 100\n",
      "277: EER = 0.4162, FMR100 = 0.7775, Count = 100\n",
      "281: EER = 0.4833, FMR100 = 0.8522, Count = 100\n",
      "282: EER = 0.3377, FMR100 = 0.5750, Count = 100\n",
      "284: EER = 0.2579, FMR100 = 0.6239, Count = 100\n",
      "286: EER = 0.2334, FMR100 = 0.6678, Count = 100\n",
      "291: EER = 0.3339, FMR100 = 0.7317, Count = 100\n",
      "296: EER = 0.2209, FMR100 = 0.7609, Count = 100\n",
      "299: EER = 0.1481, FMR100 = 0.7895, Count = 100\n",
      "301: EER = 0.0727, FMR100 = 0.4095, Count = 100\n",
      "303: EER = 0.0945, FMR100 = 0.4870, Count = 100\n",
      "306: EER = 0.2764, FMR100 = 0.9364, Count = 100\n",
      "310: EER = 0.1473, FMR100 = 0.7141, Count = 100\n",
      "314: EER = 0.1003, FMR100 = 0.4960, Count = 100\n",
      "322: EER = 0.1031, FMR100 = 0.4600, Count = 100\n",
      "325: EER = 0.1010, FMR100 = 0.5624, Count = 100\n",
      "332: EER = 0.2611, FMR100 = 0.9133, Count = 100\n",
      "336: EER = 0.1788, FMR100 = 0.9286, Count = 100\n",
      "342: EER = 0.1411, FMR100 = 0.8641, Count = 100\n",
      "348: EER = 0.1770, FMR100 = 0.8877, Count = 100\n",
      "354: EER = 0.1706, FMR100 = 0.9182, Count = 100\n",
      "357: EER = 0.1868, FMR100 = 0.9133, Count = 100\n",
      "364: EER = 0.1669, FMR100 = 0.9167, Count = 100\n",
      "365: EER = 0.1508, FMR100 = 0.9079, Count = 100\n",
      "373: EER = 0.1769, FMR100 = 0.9355, Count = 100\n",
      "374: EER = 0.1263, FMR100 = 0.8105, Count = 100\n",
      "384: EER = 0.1625, FMR100 = 0.8625, Count = 100\n",
      "385: EER = 0.1220, FMR100 = 0.7890, Count = 100\n",
      "387: EER = 0.1737, FMR100 = 0.8241, Count = 100\n",
      "402: EER = 0.2125, FMR100 = 0.8912, Count = 100\n",
      "404: EER = 0.0934, FMR100 = 0.6457, Count = 100\n",
      "407: EER = 0.2106, FMR100 = 0.9575, Count = 100\n",
      "413: EER = 0.1576, FMR100 = 0.9300, Count = 100\n",
      "414: EER = 0.1464, FMR100 = 0.8800, Count = 100\n",
      "415: EER = 0.1378, FMR100 = 0.8612, Count = 100\n",
      "418: EER = 0.2229, FMR100 = 0.9712, Count = 100\n",
      "419: EER = 0.1353, FMR100 = 0.7750, Count = 100\n",
      "422: EER = 0.1420, FMR100 = 0.8557, Count = 100\n",
      "423: EER = 0.1440, FMR100 = 0.8725, Count = 100\n",
      "433: EER = 0.1536, FMR100 = 0.9133, Count = 100\n",
      "436: EER = 0.1486, FMR100 = 0.8357, Count = 100\n",
      "Final Average EER: 26.4166\n",
      "Final EER Standard Deviation: 16.2893\n",
      "$26.42 \\pm 16.29$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26.416558026851654, 16.289313062290883)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#, s_test_b\n",
    "resutls2,resutls3=assessment_model_data_two(x_test_e,y_test_e, x_test_v,y_test_v,trunk, distance = \"cd\")\n",
    "#print(\"simple\",EERf(resutls))\n",
    "calculate_and_print_averages(y_test_e, y_test_e, resutls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5732b-e979-41f7-8216-3ab8b2fb0a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b9f7b44-fa3e-49a6-b255-92055d27d513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 70, Number of unique sessions: 12\n",
      "Subject 82, Number of unique sessions: 19\n",
      "Subject 85, Number of unique sessions: 18\n",
      "Subject 86, Number of unique sessions: 14\n",
      "Subject 96, Number of unique sessions: 9\n",
      "Subject 103, Number of unique sessions: 8\n",
      "Subject 106, Number of unique sessions: 28\n",
      "Subject 109, Number of unique sessions: 4\n",
      "Subject 111, Number of unique sessions: 13\n",
      "Subject 115, Number of unique sessions: 26\n",
      "Subject 118, Number of unique sessions: 11\n",
      "Subject 119, Number of unique sessions: 4\n",
      "Subject 125, Number of unique sessions: 11\n",
      "Subject 130, Number of unique sessions: 7\n",
      "Subject 131, Number of unique sessions: 10\n",
      "Subject 136, Number of unique sessions: 13\n",
      "Subject 145, Number of unique sessions: 10\n",
      "Subject 156, Number of unique sessions: 6\n",
      "Subject 159, Number of unique sessions: 8\n",
      "Subject 169, Number of unique sessions: 6\n",
      "Subject 173, Number of unique sessions: 6\n",
      "Subject 174, Number of unique sessions: 10\n",
      "Subject 175, Number of unique sessions: 6\n",
      "Subject 176, Number of unique sessions: 6\n",
      "Subject 181, Number of unique sessions: 5\n",
      "Subject 183, Number of unique sessions: 4\n",
      "Subject 184, Number of unique sessions: 9\n",
      "Subject 187, Number of unique sessions: 29\n",
      "Subject 191, Number of unique sessions: 9\n",
      "Subject 192, Number of unique sessions: 9\n",
      "Subject 194, Number of unique sessions: 13\n",
      "Subject 197, Number of unique sessions: 13\n",
      "Subject 199, Number of unique sessions: 13\n",
      "Subject 201, Number of unique sessions: 9\n",
      "Subject 203, Number of unique sessions: 6\n",
      "Subject 206, Number of unique sessions: 6\n",
      "Subject 207, Number of unique sessions: 30\n",
      "Subject 214, Number of unique sessions: 7\n",
      "Subject 216, Number of unique sessions: 4\n",
      "Subject 217, Number of unique sessions: 5\n",
      "Subject 219, Number of unique sessions: 4\n",
      "Subject 221, Number of unique sessions: 5\n",
      "Subject 225, Number of unique sessions: 3\n",
      "Subject 230, Number of unique sessions: 4\n",
      "Subject 231, Number of unique sessions: 11\n",
      "Subject 233, Number of unique sessions: 11\n",
      "Subject 236, Number of unique sessions: 32\n",
      "Subject 249, Number of unique sessions: 18\n",
      "Subject 250, Number of unique sessions: 27\n",
      "Subject 251, Number of unique sessions: 31\n",
      "Subject 257, Number of unique sessions: 5\n",
      "Subject 260, Number of unique sessions: 41\n",
      "Subject 261, Number of unique sessions: 4\n",
      "Subject 262, Number of unique sessions: 6\n",
      "Subject 265, Number of unique sessions: 25\n",
      "Subject 268, Number of unique sessions: 6\n",
      "Subject 270, Number of unique sessions: 13\n",
      "Subject 277, Number of unique sessions: 5\n",
      "Subject 281, Number of unique sessions: 9\n",
      "Subject 282, Number of unique sessions: 3\n",
      "Subject 284, Number of unique sessions: 12\n",
      "Subject 286, Number of unique sessions: 4\n",
      "Subject 291, Number of unique sessions: 8\n",
      "Subject 296, Number of unique sessions: 23\n",
      "Subject 299, Number of unique sessions: 22\n",
      "Subject 301, Number of unique sessions: 23\n",
      "Subject 303, Number of unique sessions: 21\n",
      "Subject 306, Number of unique sessions: 23\n",
      "Subject 310, Number of unique sessions: 23\n",
      "Subject 314, Number of unique sessions: 6\n",
      "Subject 322, Number of unique sessions: 21\n",
      "Subject 325, Number of unique sessions: 22\n",
      "x_test_e: (7200, 93, 500), y_test_e: (7200,), s_test_e: (7200,)\n",
      "x_test_v: (83572, 93, 500), y_test_v: (83572,), s_test_v: (83572,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Load test data from test_raw.h5\n",
    "with h5py.File(\"./data/hardware_HydroCe.h5\", \"r\") as f_test:\n",
    "    X_test = f_test['X'][:]\n",
    "    Y_test = f_test['Y'][:]\n",
    "    S_test = f_test['S'][:]\n",
    "    H_test = f_test['H'][:]  # Load the 'hardwares' dataset as h_test\n",
    "\n",
    "\n",
    "# Now, X_combined, Y_combined, S_combined, and h_combined contain the merged data\n",
    "\n",
    "\n",
    "# Now, X_combined, Y_combined, and S_combined contain the merged data\n",
    "\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Reorder the arrays according to the shuffled indices\n",
    "x_test = X_test[indices]\n",
    "y_test = Y_test[indices]\n",
    "s_test = S_test[indices]\n",
    "h_test = H_test[indices]\n",
    "\n",
    "\n",
    "# Find unique subjects\n",
    "unique_subjects = np.unique(y_test)\n",
    "\n",
    "# Initialize lists to hold the data for x_test_e and x_test_v\n",
    "x_test_e_list = []\n",
    "y_test_e_list = []\n",
    "s_test_e_list = []\n",
    "h_test_e_list = []\n",
    "\n",
    "x_test_v_list = []\n",
    "y_test_v_list = []\n",
    "s_test_v_list = []\n",
    "h_test_v_list = []\n",
    "\n",
    "# Assign the minimum session for each subject to x_test_e and the rest to x_test_v\n",
    "for subject in unique_subjects:\n",
    "    subject_indices = np.where(y_test == subject)[0]\n",
    "    subject_sessions = s_test[subject_indices]\n",
    "    \n",
    "    # Skip subjects with fewer than two unique sessions\n",
    "    if len(np.unique(subject_sessions)) < 2:\n",
    "        continue\n",
    "\n",
    "    print(f\"Subject {subject}, Number of unique sessions: {len(np.unique(subject_sessions))}\")\n",
    "    \n",
    "    # Assign the minimum session to the evaluation set (x_test_e)\n",
    "    min_session = np.min(subject_sessions)\n",
    "    \n",
    "    # Append data to the evaluation set (min session)\n",
    "    x_test_e_list.extend(x_test[subject_indices][subject_sessions == min_session])\n",
    "    y_test_e_list.extend(y_test[subject_indices][subject_sessions == min_session])\n",
    "    s_test_e_list.extend(s_test[subject_indices][subject_sessions == min_session])\n",
    "    h_test_e_list.extend(h_test[subject_indices][subject_sessions == min_session])\n",
    "\n",
    "    # Append remaining sessions to the validation set (x_test_v)\n",
    "    x_test_v_list.extend(x_test[subject_indices][subject_sessions != min_session])\n",
    "    y_test_v_list.extend(y_test[subject_indices][subject_sessions != min_session])\n",
    "    s_test_v_list.extend(s_test[subject_indices][subject_sessions != min_session])\n",
    "    h_test_v_list.extend(h_test[subject_indices][subject_sessions != min_session])\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_e\n",
    "indices_e = np.arange(len(x_test_e_list))\n",
    "np.random.shuffle(indices_e)\n",
    "\n",
    "x_test_e = np.array(x_test_e_list)[indices_e]\n",
    "y_test_e = np.array(y_test_e_list)[indices_e]\n",
    "s_test_e = np.array(s_test_e_list)[indices_e]\n",
    "h_test_e = np.array(h_test_e_list)[indices_e]\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_v\n",
    "indices_v = np.arange(len(x_test_v_list))\n",
    "np.random.shuffle(indices_v)\n",
    "\n",
    "x_test_v = np.array(x_test_v_list)[indices_v]\n",
    "y_test_v = np.array(y_test_v_list)[indices_v]\n",
    "s_test_v = np.array(s_test_v_list)[indices_v]\n",
    "h_test_v = np.array(h_test_v_list)[indices_v]\n",
    "# Optional: Save the new test evaluation and validation sets to npy files (if needed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"x_test_e: {x_test_e.shape}, y_test_e: {y_test_e.shape}, s_test_e: {s_test_e.shape}\")\n",
    "print(f\"x_test_v: {x_test_v.shape}, y_test_v: {y_test_v.shape}, s_test_v: {s_test_v.shape}\")\n",
    "\n",
    "\n",
    "# Load the state dictionary from the saved file\n",
    "state_dict = torch.load('./model/SupConLoss128_m4_e99_HydroCe_unconnected.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "trunk.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64319fd8-1da3-4227-a073-b819bd4e969e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings shape: (7200, 128)\n",
      "Computed embeddings shape: (83572, 128)\n",
      "70: EER = 0.1685, FMR100 = 0.7700, Count = 100\n",
      "82: EER = 0.0324, FMR100 = 0.0919, Count = 100\n",
      "85: EER = 0.1290, FMR100 = 0.6747, Count = 100\n",
      "86: EER = 0.0296, FMR100 = 0.1877, Count = 100\n",
      "96: EER = 0.0541, FMR100 = 0.3000, Count = 100\n",
      "103: EER = 0.0329, FMR100 = 0.1600, Count = 100\n",
      "106: EER = 0.2402, FMR100 = 0.6693, Count = 100\n",
      "109: EER = 0.0837, FMR100 = 0.4667, Count = 100\n",
      "111: EER = 0.1134, FMR100 = 0.6192, Count = 100\n",
      "115: EER = 0.3127, FMR100 = 0.6831, Count = 100\n",
      "118: EER = 0.0902, FMR100 = 0.3850, Count = 100\n",
      "119: EER = 0.0500, FMR100 = 0.2833, Count = 100\n",
      "125: EER = 0.0530, FMR100 = 0.1140, Count = 100\n",
      "130: EER = 0.0138, FMR100 = 0.0233, Count = 100\n",
      "131: EER = 0.3426, FMR100 = 0.9622, Count = 100\n",
      "136: EER = 0.1242, FMR100 = 0.6567, Count = 100\n",
      "145: EER = 0.0612, FMR100 = 0.4244, Count = 100\n",
      "156: EER = 0.0627, FMR100 = 0.2300, Count = 100\n",
      "159: EER = 0.0276, FMR100 = 0.0971, Count = 100\n",
      "169: EER = 0.1040, FMR100 = 0.3740, Count = 100\n",
      "173: EER = 0.1280, FMR100 = 0.6560, Count = 100\n",
      "174: EER = 0.1833, FMR100 = 0.8322, Count = 100\n",
      "175: EER = 0.0740, FMR100 = 0.3020, Count = 100\n",
      "176: EER = 0.1190, FMR100 = 0.3820, Count = 100\n",
      "181: EER = 0.1054, FMR100 = 0.7575, Count = 100\n",
      "183: EER = 0.0411, FMR100 = 0.1867, Count = 100\n",
      "184: EER = 0.0553, FMR100 = 0.3463, Count = 100\n",
      "187: EER = 0.3212, FMR100 = 0.8746, Count = 100\n",
      "191: EER = 0.1492, FMR100 = 0.7388, Count = 100\n",
      "192: EER = 0.3266, FMR100 = 0.9712, Count = 100\n",
      "194: EER = 0.1317, FMR100 = 0.6308, Count = 100\n",
      "197: EER = 0.1536, FMR100 = 0.6942, Count = 100\n",
      "199: EER = 0.0883, FMR100 = 0.4517, Count = 100\n",
      "201: EER = 0.0962, FMR100 = 0.6338, Count = 100\n",
      "203: EER = 0.0267, FMR100 = 0.0460, Count = 100\n",
      "206: EER = 0.0542, FMR100 = 0.3740, Count = 100\n",
      "207: EER = 0.1134, FMR100 = 0.5434, Count = 100\n",
      "214: EER = 0.0902, FMR100 = 0.3217, Count = 100\n",
      "216: EER = 0.0367, FMR100 = 0.1467, Count = 100\n",
      "217: EER = 0.0028, FMR100 = 0.0025, Count = 100\n",
      "219: EER = 0.0551, FMR100 = 0.2363, Count = 100\n",
      "221: EER = 0.1110, FMR100 = 0.6275, Count = 100\n",
      "225: EER = 0.0372, FMR100 = 0.2200, Count = 100\n",
      "230: EER = 0.3268, FMR100 = 0.8333, Count = 100\n",
      "231: EER = 0.1040, FMR100 = 0.5930, Count = 100\n",
      "233: EER = 0.1580, FMR100 = 0.8350, Count = 100\n",
      "236: EER = 0.4464, FMR100 = 0.8445, Count = 100\n",
      "249: EER = 0.3090, FMR100 = 0.8706, Count = 100\n",
      "250: EER = 0.1842, FMR100 = 0.8623, Count = 100\n",
      "251: EER = 0.1762, FMR100 = 0.7260, Count = 100\n",
      "257: EER = 0.0925, FMR100 = 0.5575, Count = 100\n",
      "260: EER = 0.1030, FMR100 = 0.5680, Count = 100\n",
      "261: EER = 0.1183, FMR100 = 0.6600, Count = 100\n",
      "262: EER = 0.0130, FMR100 = 0.0220, Count = 100\n",
      "265: EER = 0.2455, FMR100 = 0.9267, Count = 100\n",
      "268: EER = 0.0609, FMR100 = 0.2540, Count = 100\n",
      "270: EER = 0.2767, FMR100 = 0.9467, Count = 100\n",
      "277: EER = 0.0625, FMR100 = 0.5400, Count = 100\n",
      "281: EER = 0.0928, FMR100 = 0.3375, Count = 100\n",
      "282: EER = 0.0450, FMR100 = 0.2150, Count = 100\n",
      "284: EER = 0.0782, FMR100 = 0.5391, Count = 100\n",
      "286: EER = 0.1767, FMR100 = 0.9300, Count = 100\n",
      "291: EER = 0.0829, FMR100 = 0.4300, Count = 100\n",
      "296: EER = 0.1816, FMR100 = 0.8273, Count = 100\n",
      "299: EER = 0.1606, FMR100 = 0.8652, Count = 100\n",
      "301: EER = 0.0779, FMR100 = 0.5205, Count = 100\n",
      "303: EER = 0.0896, FMR100 = 0.5750, Count = 100\n",
      "306: EER = 0.2568, FMR100 = 0.9473, Count = 100\n",
      "310: EER = 0.1473, FMR100 = 0.8086, Count = 100\n",
      "314: EER = 0.0740, FMR100 = 0.4300, Count = 100\n",
      "322: EER = 0.1025, FMR100 = 0.5100, Count = 100\n",
      "325: EER = 0.1224, FMR100 = 0.6781, Count = 100\n",
      "Final Average EER: 12.4879\n",
      "Final EER Standard Deviation: 9.2342\n",
      "$12.49 \\pm 9.23$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12.487944901013607, 9.23419579965768)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#, s_test_b\n",
    "resutls2,resutls3=assessment_model_data_two(x_test_e,y_test_e, x_test_v,y_test_v,trunk, distance = \"cd\")\n",
    "#print(\"simple\",EERf(resutls))\n",
    "calculate_and_print_averages(y_test_e, y_test_e, resutls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb9f26-a374-439f-ac34-172dbb2112e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97d5ad18-effb-48dd-961a-6fcce00b952a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 260, Number of unique sessions: 8\n",
      "Subject 332, Number of unique sessions: 3\n",
      "Subject 336, Number of unique sessions: 23\n",
      "Subject 342, Number of unique sessions: 23\n",
      "Subject 348, Number of unique sessions: 23\n",
      "Subject 354, Number of unique sessions: 23\n",
      "Subject 357, Number of unique sessions: 22\n",
      "Subject 364, Number of unique sessions: 22\n",
      "Subject 365, Number of unique sessions: 30\n",
      "Subject 373, Number of unique sessions: 23\n",
      "Subject 374, Number of unique sessions: 20\n",
      "Subject 384, Number of unique sessions: 5\n",
      "Subject 385, Number of unique sessions: 22\n",
      "Subject 387, Number of unique sessions: 23\n",
      "Subject 402, Number of unique sessions: 9\n",
      "Subject 404, Number of unique sessions: 8\n",
      "Subject 407, Number of unique sessions: 9\n",
      "Subject 413, Number of unique sessions: 9\n",
      "Subject 414, Number of unique sessions: 9\n",
      "Subject 415, Number of unique sessions: 9\n",
      "Subject 418, Number of unique sessions: 9\n",
      "Subject 419, Number of unique sessions: 9\n",
      "Subject 422, Number of unique sessions: 8\n",
      "Subject 423, Number of unique sessions: 9\n",
      "Subject 433, Number of unique sessions: 7\n",
      "Subject 436, Number of unique sessions: 8\n",
      "x_test_e: (2600, 93, 500), y_test_e: (2600,), s_test_e: (2600,)\n",
      "x_test_v: (34700, 93, 500), y_test_v: (34700,), s_test_v: (34700,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Load test data from test_raw.h5\n",
    "with h5py.File(\"./data/hardware_BioSemi.h5\", \"r\") as f_test:\n",
    "    X_test = f_test['X'][:]\n",
    "    Y_test = f_test['Y'][:]\n",
    "    S_test = f_test['S'][:]\n",
    "    H_test = f_test['H'][:]  # Load the 'hardwares' dataset as h_test\n",
    "\n",
    "\n",
    "# Now, X_combined, Y_combined, S_combined, and h_combined contain the merged data\n",
    "\n",
    "\n",
    "# Now, X_combined, Y_combined, and S_combined contain the merged data\n",
    "\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Reorder the arrays according to the shuffled indices\n",
    "x_test = X_test[indices]\n",
    "y_test = Y_test[indices]\n",
    "s_test = S_test[indices]\n",
    "h_test = H_test[indices]\n",
    "\n",
    "\n",
    "# Find unique subjects\n",
    "unique_subjects = np.unique(y_test)\n",
    "\n",
    "# Initialize lists to hold the data for x_test_e and x_test_v\n",
    "x_test_e_list = []\n",
    "y_test_e_list = []\n",
    "s_test_e_list = []\n",
    "h_test_e_list = []\n",
    "\n",
    "x_test_v_list = []\n",
    "y_test_v_list = []\n",
    "s_test_v_list = []\n",
    "h_test_v_list = []\n",
    "\n",
    "# Assign the minimum session for each subject to x_test_e and the rest to x_test_v\n",
    "for subject in unique_subjects:\n",
    "    subject_indices = np.where(y_test == subject)[0]\n",
    "    subject_sessions = s_test[subject_indices]\n",
    "    \n",
    "    # Skip subjects with fewer than two unique sessions\n",
    "    if len(np.unique(subject_sessions)) < 2:\n",
    "        continue\n",
    "\n",
    "    print(f\"Subject {subject}, Number of unique sessions: {len(np.unique(subject_sessions))}\")\n",
    "    \n",
    "    # Assign the minimum session to the evaluation set (x_test_e)\n",
    "    min_session = np.min(subject_sessions)\n",
    "    \n",
    "    # Append data to the evaluation set (min session)\n",
    "    x_test_e_list.extend(x_test[subject_indices][subject_sessions == min_session])\n",
    "    y_test_e_list.extend(y_test[subject_indices][subject_sessions == min_session])\n",
    "    s_test_e_list.extend(s_test[subject_indices][subject_sessions == min_session])\n",
    "    h_test_e_list.extend(h_test[subject_indices][subject_sessions == min_session])\n",
    "\n",
    "    # Append remaining sessions to the validation set (x_test_v)\n",
    "    x_test_v_list.extend(x_test[subject_indices][subject_sessions != min_session])\n",
    "    y_test_v_list.extend(y_test[subject_indices][subject_sessions != min_session])\n",
    "    s_test_v_list.extend(s_test[subject_indices][subject_sessions != min_session])\n",
    "    h_test_v_list.extend(h_test[subject_indices][subject_sessions != min_session])\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_e\n",
    "indices_e = np.arange(len(x_test_e_list))\n",
    "np.random.shuffle(indices_e)\n",
    "\n",
    "x_test_e = np.array(x_test_e_list)[indices_e]\n",
    "y_test_e = np.array(y_test_e_list)[indices_e]\n",
    "s_test_e = np.array(s_test_e_list)[indices_e]\n",
    "h_test_e = np.array(h_test_e_list)[indices_e]\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_v\n",
    "indices_v = np.arange(len(x_test_v_list))\n",
    "np.random.shuffle(indices_v)\n",
    "\n",
    "x_test_v = np.array(x_test_v_list)[indices_v]\n",
    "y_test_v = np.array(y_test_v_list)[indices_v]\n",
    "s_test_v = np.array(s_test_v_list)[indices_v]\n",
    "h_test_v = np.array(h_test_v_list)[indices_v]\n",
    "# Optional: Save the new test evaluation and validation sets to npy files (if needed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"x_test_e: {x_test_e.shape}, y_test_e: {y_test_e.shape}, s_test_e: {s_test_e.shape}\")\n",
    "print(f\"x_test_v: {x_test_v.shape}, y_test_v: {y_test_v.shape}, s_test_v: {s_test_v.shape}\")\n",
    "\n",
    "\n",
    "# Load the state dictionary from the saved file\n",
    "state_dict = torch.load('./model/SupConLoss128_m4_e99_HydroCe_unconnected.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "trunk.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6cd1f08f-13d0-4cb5-a77f-c962a142000a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings shape: (2600, 128)\n",
      "Computed embeddings shape: (34700, 128)\n",
      "260: EER = 0.4407, FMR100 = 0.9614, Count = 100\n",
      "332: EER = 0.4111, FMR100 = 0.9450, Count = 100\n",
      "336: EER = 0.4215, FMR100 = 0.9836, Count = 100\n",
      "342: EER = 0.3742, FMR100 = 0.9645, Count = 100\n",
      "348: EER = 0.4304, FMR100 = 0.9705, Count = 100\n",
      "354: EER = 0.4392, FMR100 = 0.9723, Count = 100\n",
      "357: EER = 0.4600, FMR100 = 0.9767, Count = 100\n",
      "364: EER = 0.4552, FMR100 = 0.9833, Count = 100\n",
      "365: EER = 0.4203, FMR100 = 0.9814, Count = 100\n",
      "373: EER = 0.4847, FMR100 = 0.9845, Count = 100\n",
      "374: EER = 0.3306, FMR100 = 0.9379, Count = 100\n",
      "384: EER = 0.3800, FMR100 = 0.9575, Count = 100\n",
      "385: EER = 0.3134, FMR100 = 0.9333, Count = 100\n",
      "387: EER = 0.3933, FMR100 = 0.9350, Count = 100\n",
      "402: EER = 0.4650, FMR100 = 0.9712, Count = 100\n",
      "404: EER = 0.2416, FMR100 = 0.8800, Count = 100\n",
      "407: EER = 0.5343, FMR100 = 0.9875, Count = 100\n",
      "413: EER = 0.4501, FMR100 = 0.9812, Count = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fallahi/anaconda3/envs/oml/lib/python3.9/site-packages/pyeer/eer_stats.py:219: UserWarning: It is possible that you had set the wrong score type. Please consider reviewing if you are using dissimilarity or similarity scores\n",
      "  warn(\"It is possible that you had set the wrong score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414: EER = 0.3600, FMR100 = 0.9600, Count = 100\n",
      "415: EER = 0.3677, FMR100 = 0.9600, Count = 100\n",
      "418: EER = 0.5266, FMR100 = 0.9938, Count = 100\n",
      "419: EER = 0.3354, FMR100 = 0.9187, Count = 100\n",
      "422: EER = 0.3843, FMR100 = 0.9600, Count = 100\n",
      "423: EER = 0.3775, FMR100 = 0.9637, Count = 100\n",
      "433: EER = 0.3917, FMR100 = 0.9867, Count = 100\n",
      "436: EER = 0.3600, FMR100 = 0.9500, Count = 100\n",
      "Final Average EER: 40.5726\n",
      "Final EER Standard Deviation: 6.4045\n",
      "$40.57 \\pm 6.40$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40.57256258594152, 6.404491096678166)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#, s_test_b\n",
    "resutls2,resutls3=assessment_model_data_two(x_test_e,y_test_e, x_test_v,y_test_v,trunk, distance = \"cd\")\n",
    "#print(\"simple\",EERf(resutls))\n",
    "calculate_and_print_averages(y_test_e, y_test_e, resutls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038e77c-fe20-4ef2-8b5a-2d9700b0cac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7565a3df-1bf6-474d-a21a-80dbdd599c14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 70, Number of unique sessions: 7\n",
      "Subject 86, Number of unique sessions: 5\n",
      "Subject 96, Number of unique sessions: 9\n",
      "Subject 103, Number of unique sessions: 10\n",
      "Subject 106, Number of unique sessions: 13\n",
      "Subject 111, Number of unique sessions: 6\n",
      "Subject 115, Number of unique sessions: 15\n",
      "Subject 118, Number of unique sessions: 8\n",
      "Subject 119, Number of unique sessions: 4\n",
      "Subject 125, Number of unique sessions: 8\n",
      "Subject 129, Number of unique sessions: 4\n",
      "Subject 130, Number of unique sessions: 12\n",
      "Subject 131, Number of unique sessions: 9\n",
      "Subject 136, Number of unique sessions: 6\n",
      "Subject 145, Number of unique sessions: 9\n",
      "Subject 159, Number of unique sessions: 11\n",
      "Subject 167, Number of unique sessions: 2\n",
      "Subject 174, Number of unique sessions: 9\n",
      "Subject 183, Number of unique sessions: 2\n",
      "Subject 184, Number of unique sessions: 10\n",
      "Subject 187, Number of unique sessions: 3\n",
      "Subject 191, Number of unique sessions: 10\n",
      "Subject 192, Number of unique sessions: 10\n",
      "Subject 194, Number of unique sessions: 6\n",
      "Subject 197, Number of unique sessions: 6\n",
      "Subject 199, Number of unique sessions: 6\n",
      "Subject 201, Number of unique sessions: 10\n",
      "Subject 207, Number of unique sessions: 11\n",
      "Subject 214, Number of unique sessions: 11\n",
      "Subject 216, Number of unique sessions: 2\n",
      "Subject 225, Number of unique sessions: 3\n",
      "Subject 230, Number of unique sessions: 2\n",
      "Subject 231, Number of unique sessions: 8\n",
      "Subject 233, Number of unique sessions: 8\n",
      "Subject 236, Number of unique sessions: 10\n",
      "Subject 245, Number of unique sessions: 6\n",
      "Subject 249, Number of unique sessions: 24\n",
      "Subject 250, Number of unique sessions: 14\n",
      "Subject 251, Number of unique sessions: 10\n",
      "Subject 261, Number of unique sessions: 6\n",
      "Subject 265, Number of unique sessions: 15\n",
      "Subject 268, Number of unique sessions: 13\n",
      "Subject 270, Number of unique sessions: 6\n",
      "Subject 277, Number of unique sessions: 8\n",
      "Subject 281, Number of unique sessions: 10\n",
      "Subject 282, Number of unique sessions: 2\n",
      "Subject 284, Number of unique sessions: 7\n",
      "Subject 286, Number of unique sessions: 15\n",
      "Subject 291, Number of unique sessions: 11\n",
      "x_test_e: (4900, 93, 500), y_test_e: (4900,), s_test_e: (4900,)\n",
      "x_test_v: (36379, 93, 500), y_test_v: (36379,), s_test_v: (36379,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Load test data from test_raw.h5\n",
    "with h5py.File(\"./data/hardware_Geodisi.h5\", \"r\") as f_test:\n",
    "    X_test = f_test['X'][:]\n",
    "    Y_test = f_test['Y'][:]\n",
    "    S_test = f_test['S'][:]\n",
    "    H_test = f_test['H'][:]  # Load the 'hardwares' dataset as h_test\n",
    "\n",
    "\n",
    "# Now, X_combined, Y_combined, S_combined, and h_combined contain the merged data\n",
    "\n",
    "\n",
    "# Now, X_combined, Y_combined, and S_combined contain the merged data\n",
    "\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Reorder the arrays according to the shuffled indices\n",
    "x_test = X_test[indices]\n",
    "y_test = Y_test[indices]\n",
    "s_test = S_test[indices]\n",
    "h_test = H_test[indices]\n",
    "\n",
    "\n",
    "# Find unique subjects\n",
    "unique_subjects = np.unique(y_test)\n",
    "\n",
    "# Initialize lists to hold the data for x_test_e and x_test_v\n",
    "x_test_e_list = []\n",
    "y_test_e_list = []\n",
    "s_test_e_list = []\n",
    "h_test_e_list = []\n",
    "\n",
    "x_test_v_list = []\n",
    "y_test_v_list = []\n",
    "s_test_v_list = []\n",
    "h_test_v_list = []\n",
    "\n",
    "# Assign the minimum session for each subject to x_test_e and the rest to x_test_v\n",
    "for subject in unique_subjects:\n",
    "    subject_indices = np.where(y_test == subject)[0]\n",
    "    subject_sessions = s_test[subject_indices]\n",
    "    \n",
    "    # Skip subjects with fewer than two unique sessions\n",
    "    if len(np.unique(subject_sessions)) < 2:\n",
    "        continue\n",
    "\n",
    "    print(f\"Subject {subject}, Number of unique sessions: {len(np.unique(subject_sessions))}\")\n",
    "    \n",
    "    # Assign the minimum session to the evaluation set (x_test_e)\n",
    "    min_session = np.min(subject_sessions)\n",
    "    \n",
    "    # Append data to the evaluation set (min session)\n",
    "    x_test_e_list.extend(x_test[subject_indices][subject_sessions == min_session])\n",
    "    y_test_e_list.extend(y_test[subject_indices][subject_sessions == min_session])\n",
    "    s_test_e_list.extend(s_test[subject_indices][subject_sessions == min_session])\n",
    "    h_test_e_list.extend(h_test[subject_indices][subject_sessions == min_session])\n",
    "\n",
    "    # Append remaining sessions to the validation set (x_test_v)\n",
    "    x_test_v_list.extend(x_test[subject_indices][subject_sessions != min_session])\n",
    "    y_test_v_list.extend(y_test[subject_indices][subject_sessions != min_session])\n",
    "    s_test_v_list.extend(s_test[subject_indices][subject_sessions != min_session])\n",
    "    h_test_v_list.extend(h_test[subject_indices][subject_sessions != min_session])\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_e\n",
    "indices_e = np.arange(len(x_test_e_list))\n",
    "np.random.shuffle(indices_e)\n",
    "\n",
    "x_test_e = np.array(x_test_e_list)[indices_e]\n",
    "y_test_e = np.array(y_test_e_list)[indices_e]\n",
    "s_test_e = np.array(s_test_e_list)[indices_e]\n",
    "h_test_e = np.array(h_test_e_list)[indices_e]\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_v\n",
    "indices_v = np.arange(len(x_test_v_list))\n",
    "np.random.shuffle(indices_v)\n",
    "\n",
    "x_test_v = np.array(x_test_v_list)[indices_v]\n",
    "y_test_v = np.array(y_test_v_list)[indices_v]\n",
    "s_test_v = np.array(s_test_v_list)[indices_v]\n",
    "h_test_v = np.array(h_test_v_list)[indices_v]\n",
    "# Optional: Save the new test evaluation and validation sets to npy files (if needed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"x_test_e: {x_test_e.shape}, y_test_e: {y_test_e.shape}, s_test_e: {s_test_e.shape}\")\n",
    "print(f\"x_test_v: {x_test_v.shape}, y_test_v: {y_test_v.shape}, s_test_v: {s_test_v.shape}\")\n",
    "\n",
    "\n",
    "# Load the state dictionary from the saved file\n",
    "state_dict = torch.load('./model/SupConLoss128_m4_e99_HydroCe_unconnected.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "trunk.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7140d5db-4c16-4a5a-ba7c-415d37ffeb1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings shape: (4900, 128)\n",
      "Computed embeddings shape: (36379, 128)\n",
      "70: EER = 0.2407, FMR100 = 0.8767, Count = 100\n",
      "86: EER = 0.3000, FMR100 = 0.8725, Count = 100\n",
      "96: EER = 0.2862, FMR100 = 0.9413, Count = 100\n",
      "103: EER = 0.2300, FMR100 = 0.8144, Count = 100\n",
      "106: EER = 0.2369, FMR100 = 0.8615, Count = 100\n",
      "111: EER = 0.5040, FMR100 = 0.9480, Count = 100\n",
      "115: EER = 0.2839, FMR100 = 0.9057, Count = 100\n",
      "118: EER = 0.2217, FMR100 = 0.8229, Count = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fallahi/anaconda3/envs/oml/lib/python3.9/site-packages/pyeer/eer_stats.py:219: UserWarning: It is possible that you had set the wrong score type. Please consider reviewing if you are using dissimilarity or similarity scores\n",
      "  warn(\"It is possible that you had set the wrong score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119: EER = 0.5370, FMR100 = 0.9667, Count = 100\n",
      "125: EER = 0.1531, FMR100 = 0.6714, Count = 100\n",
      "129: EER = 0.6267, FMR100 = 0.9967, Count = 100\n",
      "130: EER = 0.2254, FMR100 = 0.5973, Count = 100\n",
      "131: EER = 0.4155, FMR100 = 0.8738, Count = 100\n",
      "136: EER = 0.4947, FMR100 = 0.8960, Count = 100\n",
      "145: EER = 0.2350, FMR100 = 0.8163, Count = 100\n",
      "159: EER = 0.2002, FMR100 = 0.8390, Count = 100\n",
      "167: EER = 0.3700, FMR100 = 0.9100, Count = 100\n",
      "174: EER = 0.5162, FMR100 = 0.9788, Count = 100\n",
      "183: EER = 0.0600, FMR100 = 0.4000, Count = 100\n",
      "184: EER = 0.3822, FMR100 = 0.9744, Count = 100\n",
      "187: EER = 0.3252, FMR100 = 0.9550, Count = 100\n",
      "191: EER = 0.3303, FMR100 = 0.9133, Count = 100\n",
      "192: EER = 0.3367, FMR100 = 0.8800, Count = 100\n",
      "194: EER = 0.1740, FMR100 = 0.7000, Count = 100\n",
      "197: EER = 0.4020, FMR100 = 0.9980, Count = 100\n",
      "199: EER = 0.2429, FMR100 = 0.9080, Count = 100\n",
      "201: EER = 0.2311, FMR100 = 0.6256, Count = 100\n",
      "207: EER = 0.2452, FMR100 = 0.8550, Count = 100\n",
      "214: EER = 0.2953, FMR100 = 0.9581, Count = 100\n",
      "216: EER = 0.1400, FMR100 = 0.5900, Count = 100\n",
      "225: EER = 0.2750, FMR100 = 0.9300, Count = 100\n",
      "230: EER = 0.5100, FMR100 = 0.9900, Count = 100\n",
      "231: EER = 0.2287, FMR100 = 0.9257, Count = 100\n",
      "233: EER = 0.2643, FMR100 = 0.8857, Count = 100\n",
      "236: EER = 0.3436, FMR100 = 0.9400, Count = 100\n",
      "245: EER = 0.1146, FMR100 = 0.5940, Count = 100\n",
      "249: EER = 0.3370, FMR100 = 0.9170, Count = 100\n",
      "250: EER = 0.2900, FMR100 = 0.9023, Count = 100\n",
      "251: EER = 0.1689, FMR100 = 0.7133, Count = 100\n",
      "261: EER = 0.1509, FMR100 = 0.7600, Count = 100\n",
      "265: EER = 0.2065, FMR100 = 0.8600, Count = 100\n",
      "268: EER = 0.2585, FMR100 = 0.8442, Count = 100\n",
      "270: EER = 0.2427, FMR100 = 0.9200, Count = 100\n",
      "277: EER = 0.2691, FMR100 = 0.9071, Count = 100\n",
      "281: EER = 0.2624, FMR100 = 0.8678, Count = 100\n",
      "282: EER = 0.1203, FMR100 = 0.5500, Count = 100\n",
      "284: EER = 0.3438, FMR100 = 0.9367, Count = 100\n",
      "286: EER = 0.1950, FMR100 = 0.8143, Count = 100\n",
      "291: EER = 0.3120, FMR100 = 0.9220, Count = 100\n",
      "Final Average EER: 28.8476\n",
      "Final EER Standard Deviation: 11.7899\n",
      "$28.85 \\pm 11.79$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28.84755644402648, 11.789916264981413)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#, s_test_b\n",
    "resutls2,resutls3=assessment_model_data_two(x_test_e,y_test_e, x_test_v,y_test_v,trunk, distance = \"cd\")\n",
    "#print(\"simple\",EERf(resutls))\n",
    "calculate_and_print_averages(y_test_e, y_test_e, resutls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d9448-50c7-4ddc-821b-6f6b1102f220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oml",
   "language": "python",
   "name": "oml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
