{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce84862-1426-40a3-b2d7-e14a8d866aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from sklearn.metrics.pairwise import cosine_distances as cd\n",
    "from collections import defaultdict\n",
    "from pyeer.eer_info import get_eer_stats\n",
    "from sklearn.metrics.pairwise import manhattan_distances as md\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "\n",
    "from pytorch_metric_learning.distances import LpDistance, CosineSimilarity,SNRDistance\n",
    "\n",
    "def EERf(resutls):\n",
    "    resutls= np.array(resutls)\n",
    "    genuine  = resutls[resutls[:, 1] == 1][:, 0]\n",
    "    impostor = resutls[resutls[:, 1] == 0][:, 0]\n",
    "    stats_a = get_eer_stats(genuine, impostor)\n",
    "    return(stats_a.eer,stats_a.fmr100)\n",
    "\n",
    "def calculate_and_print_averages(y_train, resutls3):\n",
    "    u, counts = np.unique(y_train, return_counts=True)\n",
    "    eer_values = []\n",
    "    ii = 0\n",
    "\n",
    "    for i in resutls3.keys():\n",
    "        re = EERf(resutls3[i])\n",
    "        eer = re[0]\n",
    "        print(f\"{i}: EER = {re[0]:.4f}, FMR100 = {re[1]:.4f}, Count = {counts[ii]}\")\n",
    "        eer_values.append(eer)\n",
    "        ii += 1\n",
    "\n",
    "    average_eer = np.mean(eer_values) * 100\n",
    "    std_eer = np.std(eer_values) * 100\n",
    "    \n",
    "    print(f\"Final Average EER: {average_eer:.4f}\")\n",
    "    print(f\"Final EER Standard Deviation: {std_eer:.4f}\")\n",
    "    print(f\"${average_eer:.2f} \\\\pm {std_eer:.2f}$\")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_similarity_scores_two(enrollment_embeddings, y_enrollment, verification_embeddings, y_verification,distance):\n",
    "    similarity_results = []\n",
    "    similarity_results_by_class = []\n",
    "    similarity_results_by_class_dict = defaultdict(list)\n",
    "    unique_classes = np.unique(y_enrollment)\n",
    "    class_indices = [np.where(y_enrollment == cls)[0] for cls in unique_classes]\n",
    "\n",
    "    if distance == \"cd\":\n",
    "        similarity_matrix = -1 * cd(verification_embeddings, enrollment_embeddings)\n",
    "    elif distance == \"ed\":\n",
    "        similarity_matrix = -1 * ed(verification_embeddings, enrollment_embeddings)\n",
    "\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        current_class = y_verification[i]\n",
    "        predicted_scores = similarity_matrix[i]\n",
    "        same_class_indices = class_indices[np.where(unique_classes == current_class)[0][0]]\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            same_class_indices = class_indices[np.where(unique_classes == cls)[0][0]]\n",
    "            max_score = sum(sorted(predicted_scores[same_class_indices], reverse=True)[:10]) / 10\n",
    "            if current_class == cls:\n",
    "                similarity_results_by_class.append([max_score, 1, current_class, cls, i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 1, current_class, cls, i, cls])\n",
    "            else:\n",
    "                similarity_results_by_class.append([max_score, 0, current_class, cls, i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 0, current_class, cls, i, cls])\n",
    "\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict\n",
    "\n",
    "def assessment_model_data_two(enrollment_data, ye, verification_data, yv, e_network, distance):\n",
    "    x_enrollment, y_enrollment = enrollment_data, ye\n",
    "    x_verification, y_verification = verification_data, yv\n",
    "    enrollment_embeddings = compute_embedding_batch_two(x_enrollment, e_network)\n",
    "    verification_embeddings = compute_embedding_batch_two(x_verification, e_network)\n",
    "    similarity_results_by_class, similarity_results_by_class_dict = calculate_similarity_scores_two(\n",
    "        enrollment_embeddings, y_enrollment, verification_embeddings, y_verification,distance\n",
    "    )\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_embedding_batch_two(x_test_batch, embedding_network, batch_size=150, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Computes embeddings for the input data in batches, including batch-wise tensor conversion.\n",
    "\n",
    "    Args:\n",
    "        x_test_batch (numpy.ndarray): Input data (in numpy format).\n",
    "        embedding_network (torch.nn.Module): The network to compute embeddings.\n",
    "        batch_size (int, optional): Size of each batch for processing. Default is 100.\n",
    "        device (str, optional): The device to use ('cuda' or 'cpu'). Default is 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Computed embeddings for the input data.\n",
    "    \"\"\"\n",
    "    total_samples = len(x_test_batch)\n",
    "    embeddings = []\n",
    "\n",
    "    # Process the data in batches\n",
    "    for start_index in range(0, total_samples, batch_size):\n",
    "        end_index = min(start_index + batch_size, total_samples)\n",
    "\n",
    "        # Convert the current batch to tensor and move to device\n",
    "        batch = torch.tensor(x_test_batch[start_index:end_index], dtype=torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            batch_embeddings = embedding_network(batch).detach().cpu().numpy()\n",
    "\n",
    "        embeddings.append(batch_embeddings)  # Store embeddings on CPU\n",
    "\n",
    "        # Clear GPU memory for the batch\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all embeddings into a single numpy array\n",
    "    anchor_embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "    print(f\"Computed embeddings shape: {anchor_embeddings.shape}\")\n",
    "    return anchor_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35b7bcf-f9bb-4330-8dfd-18e5a0b03554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EEGNet7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet7, self).__init__()\n",
    "        # InstanceNorm1d normalizes each channel across its 500 values for each sample\n",
    "        self.norm = nn.InstanceNorm1d(93, affine=False)  # Normalizes across the 500 values in each of the 93 channels\n",
    "        self.act = nn.ReLU()  # or nn.SELU() , ReLU\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.conv1 = nn.Conv2d(1, 256, (1, 4), padding='same')\n",
    "        self.pool1 = nn.MaxPool2d((1, 2))\n",
    "        self.conv2 = nn.Conv2d(256, 192, (4, 1), padding='same')\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))\n",
    "        self.conv3 = nn.Conv2d(192, 128, (1, 4), padding='same')\n",
    "        self.pool3 = nn.MaxPool2d((1, 2))\n",
    "        self.conv4 = nn.Conv2d(128, 96, (4, 1), padding='same')\n",
    "        self.pool4 = nn.MaxPool2d((2, 1))\n",
    "        self.conv5 = nn.Conv2d(96, 64, (1, 4), padding='same')\n",
    "        self.pool5 = nn.MaxPool2d((1, 2))\n",
    "        self.conv6 = nn.Conv2d(64, 32, (4, 1), padding='same')\n",
    "        self.pool6 = nn.MaxPool2d((2, 1))\n",
    "        self.conv7 = nn.Conv2d(32, 16, (1, 2), padding='same')\n",
    "        self.conv8 = nn.Conv2d(16, 2, (2, 1), padding='same')\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.embedding = nn.Linear(1364, 128)  # Embedding layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        #if x.dim() == 3:\n",
    "            #x = x.unsqueeze(1)  # Adding a channel dimension if input is 3D\n",
    "        x = self.norm(x)  # Apply InstanceNorm along the channel dimension (squeeze to 1D first)\n",
    "        x = x.unsqueeze(1)  # Reshape to 4D again for Conv2d\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.act(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.act(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.act(self.conv5(x))\n",
    "        x = self.pool5(x)\n",
    "        x = self.act(self.conv6(x))\n",
    "        x = self.pool6(x)\n",
    "        x = self.act(self.conv7(x))\n",
    "        x = self.act(self.conv8(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.embedding(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trunk = EEGNet7().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35fbfd4-efb8-4c80-afec-01cef24f09a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 70, Number of unique sessions: 19\n",
      "Subject 82, Number of unique sessions: 19\n",
      "Subject 85, Number of unique sessions: 19\n",
      "Subject 86, Number of unique sessions: 19\n",
      "Subject 96, Number of unique sessions: 18\n",
      "Subject 103, Number of unique sessions: 18\n",
      "Subject 106, Number of unique sessions: 41\n",
      "Subject 109, Number of unique sessions: 5\n",
      "Subject 111, Number of unique sessions: 19\n",
      "Subject 115, Number of unique sessions: 41\n",
      "Subject 118, Number of unique sessions: 19\n",
      "Subject 119, Number of unique sessions: 8\n",
      "Subject 125, Number of unique sessions: 19\n",
      "Subject 129, Number of unique sessions: 5\n",
      "Subject 130, Number of unique sessions: 19\n",
      "Subject 131, Number of unique sessions: 19\n",
      "Subject 136, Number of unique sessions: 19\n",
      "Subject 145, Number of unique sessions: 19\n",
      "Subject 156, Number of unique sessions: 6\n",
      "Subject 159, Number of unique sessions: 19\n",
      "Subject 167, Number of unique sessions: 2\n",
      "Subject 169, Number of unique sessions: 6\n",
      "Subject 173, Number of unique sessions: 6\n",
      "Subject 174, Number of unique sessions: 19\n",
      "Subject 175, Number of unique sessions: 6\n",
      "Subject 176, Number of unique sessions: 6\n",
      "Subject 181, Number of unique sessions: 5\n",
      "Subject 183, Number of unique sessions: 6\n",
      "Subject 184, Number of unique sessions: 19\n",
      "Subject 187, Number of unique sessions: 32\n",
      "Subject 191, Number of unique sessions: 19\n",
      "Subject 192, Number of unique sessions: 19\n",
      "Subject 194, Number of unique sessions: 19\n",
      "Subject 197, Number of unique sessions: 19\n",
      "Subject 199, Number of unique sessions: 19\n",
      "Subject 201, Number of unique sessions: 19\n",
      "Subject 203, Number of unique sessions: 6\n",
      "Subject 206, Number of unique sessions: 6\n",
      "Subject 207, Number of unique sessions: 41\n",
      "Subject 214, Number of unique sessions: 18\n",
      "Subject 216, Number of unique sessions: 6\n",
      "Subject 217, Number of unique sessions: 6\n",
      "Subject 219, Number of unique sessions: 5\n",
      "Subject 221, Number of unique sessions: 5\n",
      "Subject 225, Number of unique sessions: 6\n",
      "Subject 230, Number of unique sessions: 6\n",
      "Subject 231, Number of unique sessions: 19\n",
      "Subject 233, Number of unique sessions: 19\n",
      "Subject 236, Number of unique sessions: 42\n",
      "Subject 245, Number of unique sessions: 6\n",
      "Subject 249, Number of unique sessions: 42\n",
      "Subject 250, Number of unique sessions: 41\n",
      "Subject 251, Number of unique sessions: 41\n",
      "Subject 257, Number of unique sessions: 6\n",
      "Subject 260, Number of unique sessions: 49\n",
      "Subject 261, Number of unique sessions: 10\n",
      "Subject 262, Number of unique sessions: 6\n",
      "Subject 265, Number of unique sessions: 40\n",
      "Subject 268, Number of unique sessions: 19\n",
      "Subject 270, Number of unique sessions: 19\n",
      "Subject 277, Number of unique sessions: 13\n",
      "Subject 281, Number of unique sessions: 19\n",
      "Subject 282, Number of unique sessions: 5\n",
      "Subject 284, Number of unique sessions: 19\n",
      "Subject 286, Number of unique sessions: 19\n",
      "Subject 291, Number of unique sessions: 19\n",
      "Subject 296, Number of unique sessions: 23\n",
      "Subject 299, Number of unique sessions: 22\n",
      "Subject 301, Number of unique sessions: 23\n",
      "Subject 303, Number of unique sessions: 21\n",
      "Subject 306, Number of unique sessions: 23\n",
      "Subject 310, Number of unique sessions: 23\n",
      "Subject 314, Number of unique sessions: 6\n",
      "Subject 322, Number of unique sessions: 21\n",
      "Subject 325, Number of unique sessions: 22\n",
      "Subject 332, Number of unique sessions: 4\n",
      "Subject 336, Number of unique sessions: 23\n",
      "Subject 342, Number of unique sessions: 23\n",
      "Subject 348, Number of unique sessions: 23\n",
      "Subject 354, Number of unique sessions: 23\n",
      "Subject 357, Number of unique sessions: 22\n",
      "Subject 364, Number of unique sessions: 22\n",
      "Subject 365, Number of unique sessions: 30\n",
      "Subject 373, Number of unique sessions: 23\n",
      "Subject 374, Number of unique sessions: 20\n",
      "Subject 384, Number of unique sessions: 5\n",
      "Subject 385, Number of unique sessions: 22\n",
      "Subject 387, Number of unique sessions: 23\n",
      "Subject 402, Number of unique sessions: 9\n",
      "Subject 404, Number of unique sessions: 8\n",
      "Subject 407, Number of unique sessions: 9\n",
      "Subject 413, Number of unique sessions: 9\n",
      "Subject 414, Number of unique sessions: 9\n",
      "Subject 415, Number of unique sessions: 9\n",
      "Subject 418, Number of unique sessions: 9\n",
      "Subject 419, Number of unique sessions: 9\n",
      "Subject 422, Number of unique sessions: 8\n",
      "Subject 423, Number of unique sessions: 9\n",
      "Subject 433, Number of unique sessions: 7\n",
      "Subject 436, Number of unique sessions: 8\n",
      "x_test_e: (10000, 93, 500), y_test_e: (10000,), s_test_e: (10000,)\n",
      "x_test_v: (160051, 93, 500), y_test_v: (160051,), s_test_v: (160051,)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Models/SupConLoss128_m4_e99.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 122\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_test_v: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_test_v\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y_test_v: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test_v\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, s_test_v: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_test_v\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Load the state dictionary from the saved file\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../Models/SupConLoss128_m4_e99.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Load the state dictionary into the model\u001b[39;00m\n\u001b[1;32m    125\u001b[0m trunk\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/oml/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/oml/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/oml/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Models/SupConLoss128_m4_e99.pth'"
     ]
    }
   ],
   "source": [
    "# import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Load test data from test_raw.h5\n",
    "with h5py.File(\"../Data/test_raw.h5\", \"r\") as f_test:\n",
    "    X_test = f_test['data'][:]\n",
    "    Y_test = f_test['labels'][:]\n",
    "    S_test = f_test['sessions'][:]\n",
    "    H_test = f_test['hardwares'][:]  # Load the 'hardwares' dataset as h_test\n",
    "\n",
    "# Load negative data from neg_raw.h5\n",
    "with h5py.File(\"../Data/neg_raw.h5\", \"r\") as f_neg:\n",
    "    X_neg = f_neg['data'][:]\n",
    "    Y_neg = f_neg['labels'][:]\n",
    "    S_neg = f_neg['sessions'][:]\n",
    "    H_neg = f_neg['hardwares'][:]  # Load the 'hardwares' dataset as h_neg\n",
    "\n",
    "# Optional: Combine or use them independently depending on your requirement\n",
    "# Example of concatenating them for a combined dataset\n",
    "X_test = np.concatenate((X_test, X_neg), axis=0)\n",
    "Y_test = np.concatenate((Y_test, Y_neg), axis=0)\n",
    "S_test = np.concatenate((S_test, S_neg), axis=0)\n",
    "H_test = np.concatenate((H_test, H_neg), axis=0)\n",
    "\n",
    "# Now, X_combined, Y_combined, S_combined, and h_combined contain the merged data\n",
    "\n",
    "\n",
    "# Now, X_combined, Y_combined, and S_combined contain the merged data\n",
    "\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Reorder the arrays according to the shuffled indices\n",
    "x_test = X_test[indices]\n",
    "y_test = Y_test[indices]\n",
    "s_test = S_test[indices]\n",
    "h_test = H_test[indices]\n",
    "\n",
    "\n",
    "# Find unique subjects\n",
    "unique_subjects = np.unique(y_test)\n",
    "\n",
    "# Initialize lists to hold the data for x_test_e and x_test_v\n",
    "x_test_e_list = []\n",
    "y_test_e_list = []\n",
    "s_test_e_list = []\n",
    "h_test_e_list = []\n",
    "\n",
    "x_test_v_list = []\n",
    "y_test_v_list = []\n",
    "s_test_v_list = []\n",
    "h_test_v_list = []\n",
    "\n",
    "# Assign the minimum session for each subject to x_test_e and the rest to x_test_v\n",
    "for subject in unique_subjects:\n",
    "    subject_indices = np.where(y_test == subject)[0]\n",
    "    subject_sessions = s_test[subject_indices]\n",
    "    \n",
    "    # Skip subjects with fewer than two unique sessions\n",
    "    if len(np.unique(subject_sessions)) < 2:\n",
    "        continue\n",
    "\n",
    "    print(f\"Subject {subject}, Number of unique sessions: {len(np.unique(subject_sessions))}\")\n",
    "    \n",
    "    # Assign the minimum session to the evaluation set (x_test_e)\n",
    "    min_session = np.min(subject_sessions)\n",
    "    \n",
    "    # Append data to the evaluation set (min session)\n",
    "    x_test_e_list.extend(x_test[subject_indices][subject_sessions == min_session])\n",
    "    y_test_e_list.extend(y_test[subject_indices][subject_sessions == min_session])\n",
    "    s_test_e_list.extend(s_test[subject_indices][subject_sessions == min_session])\n",
    "    h_test_e_list.extend(h_test[subject_indices][subject_sessions == min_session])\n",
    "\n",
    "    # Append remaining sessions to the validation set (x_test_v)\n",
    "    x_test_v_list.extend(x_test[subject_indices][subject_sessions != min_session])\n",
    "    y_test_v_list.extend(y_test[subject_indices][subject_sessions != min_session])\n",
    "    s_test_v_list.extend(s_test[subject_indices][subject_sessions != min_session])\n",
    "    h_test_v_list.extend(h_test[subject_indices][subject_sessions != min_session])\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_e\n",
    "indices_e = np.arange(len(x_test_e_list))\n",
    "np.random.shuffle(indices_e)\n",
    "\n",
    "x_test_e = np.array(x_test_e_list)[indices_e]\n",
    "y_test_e = np.array(y_test_e_list)[indices_e]\n",
    "s_test_e = np.array(s_test_e_list)[indices_e]\n",
    "h_test_e = np.array(h_test_v_list)[indices_e]\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_v\n",
    "indices_v = np.arange(len(x_test_v_list))\n",
    "np.random.shuffle(indices_v)\n",
    "\n",
    "x_test_v = np.array(x_test_v_list)[indices_v]\n",
    "y_test_v = np.array(y_test_v_list)[indices_v]\n",
    "s_test_v = np.array(s_test_v_list)[indices_v]\n",
    "h_test_v = np.array(h_test_v_list)[indices_v]\n",
    "# Optional: Save the new test evaluation and validation sets to npy files (if needed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"x_test_e: {x_test_e.shape}, y_test_e: {y_test_e.shape}, s_test_e: {s_test_e.shape}\")\n",
    "print(f\"x_test_v: {x_test_v.shape}, y_test_v: {y_test_v.shape}, s_test_v: {s_test_v.shape}\")\n",
    "\n",
    "\n",
    "# Load the state dictionary from the saved file\n",
    "state_dict = torch.load('../PreTrained_Models/SupConLoss128_m4_e99.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "trunk.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "x_test_ee = compute_embedding_batch_two(x_test_e, trunk)\n",
    "x_test_ve = compute_embedding_batch_two(x_test_v, trunk)\n",
    "\n",
    "np.save('./files/x_test_e.npy', x_test_ee)\n",
    "np.save('./files/y_test_e.npy', y_test_e)\n",
    "np.save('./files/s_test_e.npy', s_test_e)\n",
    "np.save('./files/h_test_e.npy', h_test_e)\n",
    "\n",
    "\n",
    "np.save('./files/x_test_v.npy', x_test_vv)\n",
    "np.save('./files/y_test_v.npy', y_test_v)\n",
    "np.save('./files/s_test_v.npy', s_test_v)\n",
    "np.save('./files/h_test_v.npy', h_test_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae803ccf-c0f2-40fc-a831-625e3f56cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128) (10000,) (10000,) (10000,)\n",
      "(160051, 128) (160051,) (160051,) (160051,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the files with the same names\n",
    "x_test_e = np.load('./files/x_test_e.npy')\n",
    "y_test_e = np.load('./files/y_test_e.npy')\n",
    "s_test_e = np.load('./files/s_test_e.npy')\n",
    "h_test_e = np.load('./files/h_test_e.npy')\n",
    "\n",
    "x_test_v = np.load('./files/x_test_v.npy')\n",
    "y_test_v = np.load('./files/y_test_v.npy')\n",
    "s_test_v = np.load('./files/s_test_v.npy')\n",
    "h_test_v = np.load('./files/h_test_v.npy')\n",
    "\n",
    "# Verify loaded arrays (optional)\n",
    "print(x_test_e.shape, y_test_e.shape, s_test_e.shape, h_test_e.shape)\n",
    "print(x_test_v.shape, y_test_v.shape, s_test_v.shape, h_test_v.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Load the state dictionary from the saved file\n",
    "state_dict = torch.load('../PreTrained_Models/SupConLoss128_m4_e99.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "trunk.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_embedding_batch_two(x_test_batch, embedding_network, batch_size=100, device=\"cuda\"):\n",
    "    print(x_test_batch.shape)\n",
    "    return x_test_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d5b52a-c88c-4b55-bdcc-df563681928b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n",
      "(160051, 128)\n",
      "70: EER = 0.0772, FMR100 = 0.3317, Count = 100\n",
      "82: EER = 0.0366, FMR100 = 0.1159, Count = 100\n",
      "85: EER = 0.1061, FMR100 = 0.5233, Count = 100\n",
      "86: EER = 0.0356, FMR100 = 0.2022, Count = 100\n",
      "96: EER = 0.1071, FMR100 = 0.4800, Count = 100\n",
      "103: EER = 0.0326, FMR100 = 0.1724, Count = 100\n",
      "106: EER = 0.2263, FMR100 = 0.6778, Count = 100\n",
      "109: EER = 0.0725, FMR100 = 0.3250, Count = 100\n",
      "111: EER = 0.0933, FMR100 = 0.5828, Count = 100\n",
      "115: EER = 0.2210, FMR100 = 0.7478, Count = 100\n",
      "118: EER = 0.0978, FMR100 = 0.5056, Count = 100\n",
      "119: EER = 0.1477, FMR100 = 0.7000, Count = 100\n",
      "125: EER = 0.0763, FMR100 = 0.3122, Count = 100\n",
      "129: EER = 0.1700, FMR100 = 0.6875, Count = 100\n",
      "130: EER = 0.0378, FMR100 = 0.0800, Count = 100\n",
      "131: EER = 0.1640, FMR100 = 0.8961, Count = 100\n",
      "136: EER = 0.1094, FMR100 = 0.7189, Count = 100\n",
      "145: EER = 0.1347, FMR100 = 0.6828, Count = 100\n",
      "156: EER = 0.0360, FMR100 = 0.2180, Count = 100\n",
      "159: EER = 0.0517, FMR100 = 0.2467, Count = 100\n",
      "167: EER = 0.1500, FMR100 = 0.8300, Count = 100\n",
      "169: EER = 0.1226, FMR100 = 0.5980, Count = 100\n",
      "173: EER = 0.0940, FMR100 = 0.2820, Count = 100\n",
      "174: EER = 0.2017, FMR100 = 0.8967, Count = 100\n",
      "175: EER = 0.0286, FMR100 = 0.1440, Count = 100\n",
      "176: EER = 0.1120, FMR100 = 0.3460, Count = 100\n",
      "181: EER = 0.1334, FMR100 = 0.8000, Count = 100\n",
      "183: EER = 0.0749, FMR100 = 0.5020, Count = 100\n",
      "184: EER = 0.0890, FMR100 = 0.6661, Count = 100\n",
      "187: EER = 0.2120, FMR100 = 0.8806, Count = 100\n",
      "191: EER = 0.1394, FMR100 = 0.6039, Count = 100\n",
      "192: EER = 0.3769, FMR100 = 0.9428, Count = 100\n",
      "194: EER = 0.1079, FMR100 = 0.7228, Count = 100\n",
      "197: EER = 0.1378, FMR100 = 0.6150, Count = 100\n",
      "199: EER = 0.1474, FMR100 = 0.7011, Count = 100\n",
      "201: EER = 0.0878, FMR100 = 0.5344, Count = 100\n",
      "203: EER = 0.0209, FMR100 = 0.0320, Count = 100\n",
      "206: EER = 0.0344, FMR100 = 0.1520, Count = 100\n",
      "207: EER = 0.0847, FMR100 = 0.6365, Count = 100\n",
      "214: EER = 0.0746, FMR100 = 0.4163, Count = 100\n",
      "216: EER = 0.1185, FMR100 = 0.3740, Count = 100\n",
      "217: EER = 0.0384, FMR100 = 0.3260, Count = 100\n",
      "219: EER = 0.0461, FMR100 = 0.2629, Count = 100\n",
      "221: EER = 0.0482, FMR100 = 0.2325, Count = 100\n",
      "225: EER = 0.0925, FMR100 = 0.5340, Count = 100\n",
      "230: EER = 0.2327, FMR100 = 0.8680, Count = 100\n",
      "231: EER = 0.1141, FMR100 = 0.6106, Count = 100\n",
      "233: EER = 0.1274, FMR100 = 0.8483, Count = 100\n",
      "236: EER = 0.3788, FMR100 = 0.8941, Count = 100\n",
      "245: EER = 0.0460, FMR100 = 0.1960, Count = 100\n",
      "249: EER = 0.1742, FMR100 = 0.7746, Count = 100\n",
      "250: EER = 0.2250, FMR100 = 0.7815, Count = 100\n",
      "251: EER = 0.1571, FMR100 = 0.6500, Count = 100\n",
      "257: EER = 0.1206, FMR100 = 0.8560, Count = 100\n",
      "260: EER = 0.2211, FMR100 = 0.7892, Count = 100\n",
      "261: EER = 0.0989, FMR100 = 0.6144, Count = 100\n",
      "262: EER = 0.0203, FMR100 = 0.0320, Count = 100\n",
      "265: EER = 0.1901, FMR100 = 0.9300, Count = 100\n",
      "268: EER = 0.1394, FMR100 = 0.5844, Count = 100\n",
      "270: EER = 0.2517, FMR100 = 0.9528, Count = 100\n",
      "277: EER = 0.0650, FMR100 = 0.3550, Count = 100\n",
      "281: EER = 0.1111, FMR100 = 0.7056, Count = 100\n",
      "282: EER = 0.0726, FMR100 = 0.3650, Count = 100\n",
      "284: EER = 0.1211, FMR100 = 0.6406, Count = 100\n",
      "286: EER = 0.0690, FMR100 = 0.3089, Count = 100\n",
      "291: EER = 0.0817, FMR100 = 0.4417, Count = 100\n",
      "296: EER = 0.2105, FMR100 = 0.9305, Count = 100\n",
      "299: EER = 0.0943, FMR100 = 0.5086, Count = 100\n",
      "301: EER = 0.0545, FMR100 = 0.3950, Count = 100\n",
      "303: EER = 0.0707, FMR100 = 0.4550, Count = 100\n",
      "306: EER = 0.1233, FMR100 = 0.9327, Count = 100\n",
      "310: EER = 0.0618, FMR100 = 0.4050, Count = 100\n",
      "314: EER = 0.0622, FMR100 = 0.3380, Count = 100\n",
      "322: EER = 0.0615, FMR100 = 0.3630, Count = 100\n",
      "325: EER = 0.0471, FMR100 = 0.2214, Count = 100\n",
      "332: EER = 0.1671, FMR100 = 0.3500, Count = 100\n",
      "336: EER = 0.1601, FMR100 = 0.8582, Count = 100\n",
      "342: EER = 0.0777, FMR100 = 0.5423, Count = 100\n",
      "348: EER = 0.1665, FMR100 = 0.8891, Count = 100\n",
      "354: EER = 0.0709, FMR100 = 0.4459, Count = 100\n",
      "357: EER = 0.1495, FMR100 = 0.8319, Count = 100\n",
      "364: EER = 0.0830, FMR100 = 0.5214, Count = 100\n",
      "365: EER = 0.1139, FMR100 = 0.7534, Count = 100\n",
      "373: EER = 0.0678, FMR100 = 0.4000, Count = 100\n",
      "374: EER = 0.0642, FMR100 = 0.5326, Count = 100\n",
      "384: EER = 0.0955, FMR100 = 0.6500, Count = 100\n",
      "385: EER = 0.0668, FMR100 = 0.4124, Count = 100\n",
      "387: EER = 0.0582, FMR100 = 0.2809, Count = 100\n",
      "402: EER = 0.0628, FMR100 = 0.4875, Count = 100\n",
      "404: EER = 0.0386, FMR100 = 0.1429, Count = 100\n",
      "407: EER = 0.1175, FMR100 = 0.7312, Count = 100\n",
      "413: EER = 0.1131, FMR100 = 0.7063, Count = 100\n",
      "414: EER = 0.0477, FMR100 = 0.2687, Count = 100\n",
      "415: EER = 0.0265, FMR100 = 0.0800, Count = 100\n",
      "418: EER = 0.0656, FMR100 = 0.4775, Count = 100\n",
      "419: EER = 0.0913, FMR100 = 0.7350, Count = 100\n",
      "422: EER = 0.0775, FMR100 = 0.5700, Count = 100\n",
      "423: EER = 0.0480, FMR100 = 0.3088, Count = 100\n",
      "433: EER = 0.0539, FMR100 = 0.3400, Count = 100\n",
      "436: EER = 0.0886, FMR100 = 0.5186, Count = 100\n",
      "Final Average EER: 10.7840\n",
      "Final EER Standard Deviation: 6.6733\n",
      "$10.78 \\pm 6.67$\n"
     ]
    }
   ],
   "source": [
    "resutls2,resutls3=assessment_model_data_two(x_test_e,y_test_e, x_test_v,y_test_v,trunk, distance = \"cd\")\n",
    "#print(\"simple\",EERf(resutls))\n",
    "calculate_and_print_averages(y_test_e, resutls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191523f-87f7-4e77-b777-4c4ee508b6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9adfcd1-1479-43a7-95aa-8c11a2d6ed7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb0054-04e6-49f1-addd-8de35b6eed31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f651d69-757b-43a3-9d37-5b699d94ef80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9a766-e090-4c36-a633-5d8c711d2273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oml",
   "language": "python",
   "name": "oml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
