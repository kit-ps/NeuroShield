{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33671977-7d05-4761-8af6-731544728f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Found: Pattern\n",
      "Train files: 4906\n",
      "Validation files: 272\n",
      "Test files: 858\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Function to extract metadata from filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r\"subject_(\\d+)_session_(\\d+)_task_(\\w+)_date_(\\d{4}-\\d{2}-\\d{2})_hardware_(\\w+)_segments.h5\"\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        subject = int(match.group(1))\n",
    "        session = int(match.group(2))\n",
    "        task = match.group(3)\n",
    "        date = match.group(4)\n",
    "        hardware = match.group(5)\n",
    "        return subject, session, task, date, hardware\n",
    "    else:\n",
    "        print(\"NOT Found: Pattern\")\n",
    "    return None, None, None, None, None\n",
    "\n",
    "# List files and extract metadata\n",
    "def list_files_and_extract_metadata(directory):\n",
    "    filenames = os.listdir(directory)\n",
    "    data_info = [extract_info_from_filename(f) for f in filenames]\n",
    "    return filenames, data_info\n",
    "\n",
    "# Group files by subject and session\n",
    "def group_files_by_subject(data_info):\n",
    "    subjects_sessions = defaultdict(list)\n",
    "    for subject, session, task, date, hardware in data_info:\n",
    "        if subject is not None:\n",
    "            subjects_sessions[subject].append((session, task, date, hardware))\n",
    "    return subjects_sessions\n",
    "\n",
    "# Main workflow\n",
    "directory = './processed_data/'\n",
    "\n",
    "# Step 1: List files and extract metadata\n",
    "filenames, data_info = list_files_and_extract_metadata(directory)\n",
    "\n",
    "# Step 2: Group the files by subject and session\n",
    "subjects_sessions = group_files_by_subject(data_info)\n",
    "\n",
    "# Step 3: Split into multi-session and single-session subjects\n",
    "multi_session_subjects = {s: sessions for s, sessions in subjects_sessions.items() if len(sessions) > 1}\n",
    "single_session_subjects = {s: sessions for s, sessions in subjects_sessions.items() if len(sessions) == 1}\n",
    "\n",
    "# Step 4: Select 50 test subjects and 10 validation subjects from multi-session subjects\n",
    "random.seed(42)\n",
    "test_subjects = random.sample(list(multi_session_subjects.keys()), 50)\n",
    "remaining_subjects = [s for s in multi_session_subjects if s not in test_subjects]\n",
    "validation_subjects = random.sample(remaining_subjects, 10)\n",
    "train_subjects = [s for s in remaining_subjects if s not in validation_subjects]\n",
    "\n",
    "# Add single-session subjects to the training set\n",
    "train_subjects.extend(single_session_subjects.keys())\n",
    "\n",
    "# Function to group files by subject lists\n",
    "def group_files_by_split(subject_list, data_info, filenames):\n",
    "    files = []\n",
    "    for subject in subject_list:\n",
    "        files.extend([filenames[i] for i, (s, _, _, _, _) in enumerate(data_info) if s == subject])\n",
    "    return files\n",
    "\n",
    "# Step 5: Group the files for each split (train, validation, and test)\n",
    "train_files = group_files_by_split(train_subjects, data_info, filenames)\n",
    "valid_files = group_files_by_split(validation_subjects, data_info, filenames)\n",
    "test_files = group_files_by_split(test_subjects, data_info, filenames)\n",
    "\n",
    "print(f\"Train files: {len(train_files)}\")\n",
    "print(f\"Validation files: {len(valid_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00eab052-f6f5-4877-8bf1-88e05d463265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_70_session_1_task_ltpFR_date_2010-02-15_hardware_Geodisi_segments.h5\n",
      "subject_70_session_2_task_ltpFR_date_2010-02-16_hardware_HydroCe_segments.h5\n",
      "subject_70_session_3_task_ltpFR_date_2010-02-22_hardware_Geodisi_segments.h5\n",
      "subject_70_session_4_task_ltpFR_date_2010-02-23_hardware_HydroCe_segments.h5\n",
      "subject_70_session_5_task_ltpFR_date_2010-02-26_hardware_HydroCe_segments.h5\n",
      "subject_70_session_6_task_ltpFR_date_2010-03-15_hardware_HydroCe_segments.h5\n",
      "subject_70_session_7_task_ltpFR_date_2010-03-16_hardware_HydroCe_segments.h5\n",
      "subject_70_session_8_task_ltpFR_date_2010-03-19_hardware_HydroCe_segments.h5\n",
      "subject_70_session_9_task_ltpFR_date_2010-03-22_hardware_Geodisi_segments.h5\n",
      "subject_70_session_10_task_ltpFR_date_2010-03-23_hardware_HydroCe_segments.h5\n",
      "subject_70_session_11_task_ltpFR_date_2010-03-29_hardware_Geodisi_segments.h5\n",
      "subject_70_session_12_task_ltpFR_date_2010-03-30_hardware_HydroCe_segments.h5\n",
      "subject_70_session_13_task_ltpFR_date_2010-04-02_hardware_Geodisi_segments.h5\n",
      "subject_70_session_14_task_ltpFR_date_2010-04-05_hardware_Geodisi_segments.h5\n",
      "subject_70_session_15_task_ltpFR_date_2010-04-06_hardware_HydroCe_segments.h5\n",
      "subject_70_session_16_task_ltpFR_date_2010-04-09_hardware_HydroCe_segments.h5\n",
      "subject_70_session_17_task_ltpFR_date_2010-04-12_hardware_HydroCe_segments.h5\n",
      "subject_70_session_18_task_ltpFR_date_2010-04-19_hardware_Geodisi_segments.h5\n",
      "subject_70_session_19_task_ltpFR_date_2010-04-20_hardware_HydroCe_segments.h5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to extract metadata from filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r\"subject_(\\d+)_session_(\\d+)_task_(\\w+)_date_(\\d{4}-\\d{2}-\\d{2})_hardware_(\\w+)_segments.h5\"\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        subject = int(match.group(1))\n",
    "        session = int(match.group(2))\n",
    "        task = match.group(3)\n",
    "        date = match.group(4)\n",
    "        hardware = match.group(5)\n",
    "        return subject, session, task, date, hardware\n",
    "    else:\n",
    "        print(f\"NOT Found: Pattern for {filename}\")\n",
    "    return None, None, None, None, None\n",
    "\n",
    "# Function to filter and sort filenames\n",
    "def filter_and_sort_filenames(filenames, specific_subject):\n",
    "    # Extract metadata and filter by subject\n",
    "    filtered_files = [\n",
    "        (filename, extract_info_from_filename(filename))\n",
    "        for filename in filenames\n",
    "    ]\n",
    "    filtered_files = [\n",
    "        (filename, info) for filename, info in filtered_files\n",
    "        if info[0] == specific_subject\n",
    "    ]\n",
    "\n",
    "    # Sort filtered files by date\n",
    "    sorted_files = sorted(\n",
    "        filtered_files,\n",
    "        key=lambda x: datetime.strptime(x[1][3], '%Y-%m-%d')  # Sort by date\n",
    "    )\n",
    "\n",
    "    # Print sorted filenames\n",
    "    for filename, info in sorted_files:\n",
    "        print(filename)\n",
    "\n",
    "\n",
    "# Filter and sort for subject 1\n",
    "specific_subject = 1\n",
    "filter_and_sort_filenames(train_files, 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714544a7-2f41-4a40-a0a4-cdf3bf3ca2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Found: Pattern\n",
      "single session number 7\n",
      "Train files: 4009\n",
      "Validation files: 315\n",
      "Negative files: 846\n",
      "Test files: 859\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Function to extract metadata from filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r\"subject_(\\d+)_session_(\\d+)_task_(\\w+)_date_(\\d{4}-\\d{2}-\\d{2})_hardware_(\\w+)_segments.h5\"\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        subject = int(match.group(1))\n",
    "        session = int(match.group(2))\n",
    "        task = match.group(3)\n",
    "        date = match.group(4)\n",
    "        hardware = match.group(5)\n",
    "        return subject, session, task, date, hardware\n",
    "    else:\n",
    "        print(\"NOT Found: Pattern\")\n",
    "    return None, None, None, None, None\n",
    "\n",
    "# List files and extract metadata\n",
    "def list_files_and_extract_metadata(directory):\n",
    "    filenames = os.listdir(directory)\n",
    "    data_info = [extract_info_from_filename(f) for f in filenames]\n",
    "    return filenames, data_info\n",
    "\n",
    "# Group files by subject and session\n",
    "def group_files_by_subject(data_info):\n",
    "    subjects_sessions = defaultdict(list)\n",
    "    for subject, session, task, date, hardware in data_info:\n",
    "        if subject is not None:\n",
    "            subjects_sessions[subject].append((session, task, date, hardware))\n",
    "    return subjects_sessions\n",
    "\n",
    "# Main workflow\n",
    "directory = './processed_data/'\n",
    "\n",
    "# Step 1: List files and extract metadata\n",
    "filenames, data_info = list_files_and_extract_metadata(directory)\n",
    "\n",
    "# Step 2: Group the files by subject and session\n",
    "subjects_sessions = group_files_by_subject(data_info)\n",
    "\n",
    "# Step 3: Split into multi-session and single-session subjects\n",
    "multi_session_subjects = {s: sessions for s, sessions in subjects_sessions.items() if len(sessions) > 1}\n",
    "single_session_subjects = {s: sessions for s, sessions in subjects_sessions.items() if len(sessions) == 1}\n",
    "\n",
    "# Step 4: Select 50 test subjects and 10 validation subjects from multi-session subjects\n",
    "random.seed(42)\n",
    "test_subjects = random.sample(list(multi_session_subjects.keys()), 100)\n",
    "remaining_subjects = [s for s in multi_session_subjects if s not in test_subjects]\n",
    "validation_subjects = random.sample(remaining_subjects, 15)\n",
    "train_subjects = [s for s in remaining_subjects if s not in validation_subjects]\n",
    "\n",
    "# Add single-session subjects to the training set\n",
    "#train_subjects.extend(single_session_subjects.keys())\n",
    "print(\"single session number\", len(single_session_subjects.keys()))\n",
    "\n",
    "# Function to group files by subject lists\n",
    "def group_files_by_split(subject_list, data_info, filenames):\n",
    "    files = []\n",
    "    for subject in subject_list:\n",
    "        files.extend([filenames[i] for i, (s, _, _, _, _) in enumerate(data_info) if s == subject])\n",
    "    return files\n",
    "\n",
    "# Step 5: Group the files for each split (train, validation, and test)\n",
    "train_files = group_files_by_split(train_subjects, data_info, filenames)\n",
    "valid_files = group_files_by_split(validation_subjects, data_info, filenames)\n",
    "Negative_files = group_files_by_split(test_subjects[:50], data_info, filenames)\n",
    "test_files = group_files_by_split(test_subjects[50:], data_info, filenames)\n",
    "\n",
    "print(f\"Train files: {len(train_files)}\")\n",
    "print(f\"Validation files: {len(valid_files)}\")\n",
    "print(f\"Negative files: {len(Negative_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8418aac-b8a9-458c-ab63-d5ec9c2f7212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 7, 'ltpFR', '2010-10-27', 'HydroCe')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info_from_filename(train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f465d1-416b-4cb6-b61a-d63ec2f4ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def create_label(subject, session, task, date, hardware):\n",
    "    # You can use different strategies here to create labels.\n",
    "    return subject  # This is currently returning only the subject ID\n",
    "\n",
    "def concatenate_h5_files(file_list, directory, output_file):\n",
    "    \"\"\"\n",
    "    Concatenates multiple h5 files from a directory into a single h5 file.\n",
    "    Additionally saves session, task, date, and hardware information.\n",
    "    \n",
    "    file_list: List of input h5 filenames (without full path).\n",
    "    directory: Directory where the h5 files are located.\n",
    "    output_file: Path to the output h5 file.\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "    all_sessions = []\n",
    "    all_tasks = []\n",
    "    all_dates = []\n",
    "    all_hardwares = []\n",
    "\n",
    "    # Initialize output file and datasets\n",
    "    with h5py.File(output_file, 'w') as f_out:\n",
    "        data_dset = None  # To hold the dataset for the concatenated data\n",
    "\n",
    "        for file in file_list:\n",
    "            # Create the full path for the file\n",
    "            full_path = os.path.join(directory, file)\n",
    "\n",
    "            # Check if the file exists\n",
    "            if not os.path.exists(full_path):\n",
    "                print(f\"Warning: File {full_path} does not exist. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            with h5py.File(full_path, 'r') as f:\n",
    "                # Assuming 'data_segments' or 'data' as keys for EEG data\n",
    "                if 'data_segments' in f:\n",
    "                    data = f['data_segments'][:]\n",
    "                elif 'data' in f:\n",
    "                    data = f['data'][:]\n",
    "                else:\n",
    "                    print(f\"Warning: No valid data found in {file}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Check if the data has the expected number of dimensions\n",
    "                if data.ndim != 3:\n",
    "                    print(f\"Skipping file {file} due to unexpected data shape: {data.shape}\")\n",
    "                    continue\n",
    "\n",
    "                # Initialize the dataset in the output file with the correct shape only once\n",
    "                if data_dset is None:\n",
    "                    all_data_shape = (0, data.shape[1], data.shape[2])  # Use the shape from the first file\n",
    "                    data_dset = f_out.create_dataset(\n",
    "                        'data', shape=all_data_shape, maxshape=(None, data.shape[1], data.shape[2]), chunks=True\n",
    "                    )\n",
    "                \n",
    "                # Resize the dataset to accommodate new data\n",
    "                current_size = data_dset.shape[0]\n",
    "                new_size = current_size + data.shape[0]\n",
    "                data_dset.resize(new_size, axis=0)\n",
    "                data_dset[current_size:new_size] = data  # Append the new data\n",
    "\n",
    "                # Extract metadata from the filename and generate labels\n",
    "                subject, session, task, date, hardware = extract_info_from_filename(file)\n",
    "                label = create_label(subject, session, task, date, hardware)\n",
    "\n",
    "                # Extend metadata lists\n",
    "                all_labels.extend([label] * len(data))  # Extend label list\n",
    "                all_sessions.extend([session] * len(data))  # Save session for each data point\n",
    "                all_tasks.extend([task] * len(data))  # Save task for each data point\n",
    "                all_dates.extend([date] * len(data))  # Save date for each data point\n",
    "                all_hardwares.extend([hardware] * len(data))  # Save hardware for each data point\n",
    "\n",
    "        # Now write all the metadata once\n",
    "        if len(all_labels) > 0:\n",
    "            f_out.create_dataset('labels', data=np.array(all_labels))\n",
    "            f_out.create_dataset('sessions', data=np.array(all_sessions))\n",
    "            f_out.create_dataset('tasks', data=np.array(all_tasks, dtype=\"S\"))  # Save task as string\n",
    "            f_out.create_dataset('dates', data=np.array(all_dates, dtype=\"S\"))  # Save date as string\n",
    "            f_out.create_dataset('hardwares', data=np.array(all_hardwares, dtype=\"S\"))  # Save hardware as string\n",
    "\n",
    "        print(f\"Successfully created {output_file} with shape: {data_dset.shape}, labels: {len(all_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71fe59c1-cf0b-4a8f-b462-42fdadecaf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created validation.h5 with shape: (31464, 93, 500), labels: 31464\n"
     ]
    }
   ],
   "source": [
    "# Directory where the files are stored\n",
    "directory = './processed_data/'\n",
    "\n",
    "concatenate_h5_files(valid_files, directory, '../Data/validation.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d5b666-b57e-436a-805f-a8efb9aff579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file subject_103_session_19_task_ltpFR_date_2010-12-10_hardware_Geodisi_segments.h5 due to unexpected data shape: (0,)\n",
      "Successfully created Negative.h5 with shape: (84408, 93, 500), labels: 84408\n"
     ]
    }
   ],
   "source": [
    "concatenate_h5_files(Negative_files, directory, '../Data/Negative.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93103f6-2204-4b85-9af1-e2cce09ead9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file subject_107_session_17_task_ltpFR_date_2010-12-17_hardware_Geodisi_segments.h5 due to unexpected data shape: (0,)\n",
      "Skipping file subject_107_session_18_task_ltpFR_date_2010-12-20_hardware_Geodisi_segments.h5 due to unexpected data shape: (0,)\n",
      "Skipping file subject_107_session_19_task_ltpFR_date_2010-12-21_hardware_HydroCe_segments.h5 due to unexpected data shape: (0,)\n",
      "Skipping file subject_98_session_17_task_ltpFR_date_2010-12-10_hardware_Geodisi_segments.h5 due to unexpected data shape: (0,)\n",
      "Skipping file subject_164_session_2_task_ltpFR_date_2011-10-07_hardware_HydroCe_segments.h5 due to unexpected data shape: (0,)\n",
      "Successfully created train.h5 with shape: (400236, 93, 500), labels: 400236\n"
     ]
    }
   ],
   "source": [
    "concatenate_h5_files(train_files, directory, '../Data/train.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd27fa67-0694-4589-9d8f-6765e3056fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file subject_322_session_16_task_ltpFR2_date_2016-04-06_hardware_HydroCe_segments.h5 due to unexpected data shape: (0,)\n",
      "Skipping file subject_96_session_15_task_ltpFR_date_2010-11-08_hardware_Geodisi_segments.h5 due to unexpected data shape: (0,)\n",
      "Successfully created test.h5 with shape: (85643, 93, 500), labels: 85643\n"
     ]
    }
   ],
   "source": [
    "concatenate_h5_files(test_files, directory, '../Data/test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f1896-df52-45d6-84d3-6349d87eb5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afeb770-4900-4de5-af32-d3d50ce8fef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d5db1-989d-4de0-a00a-e102520788dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab083f7-a212-4bf2-ae46-7946a7f56d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a1b65-5251-4d33-9db5-1582dcd48f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a71657-0893-493e-a3aa-60ccabdbfe18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5462f-8fdd-4822-859a-292dc8acee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def inspect_h5_file(h5_file_path):\n",
    "    \"\"\"\n",
    "    Inspects the structure of an HDF5 file and prints all datasets.\n",
    "    \n",
    "    Args:\n",
    "    - h5_file_path: Path to the HDF5 file to inspect.\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        print(\"Datasets in the file:\")\n",
    "        f.visititems(lambda name, obj: print(f\"{name}: {obj}\"))\n",
    "\n",
    "# Inspect the file structure\n",
    "h5_file_path = 'validation.h5'\n",
    "inspect_h5_file(h5_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a273f2e5-9f6e-4834-b33c-b2e8e3c44478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subjects: 292\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, h5_file_path):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with data from the specified HDF5 file.\n",
    "        Args:\n",
    "        - h5_file_path: Path to the HDF5 file containing the data and labels.\n",
    "        \"\"\"\n",
    "        # Open the HDF5 file\n",
    "        self.h5_file = h5py.File(h5_file_path, 'r')\n",
    "\n",
    "        # Access the data and labels from the HDF5 file\n",
    "        self.x_data = self.h5_file['data']  # EEG data (use memory-mapped access)\n",
    "        self.y_data = self.h5_file['labels'][:]  # Labels (load fully into memory)\n",
    "        self.s_data = self.h5_file['sessions'][:]  # Sessions (load fully into memory)\n",
    "\n",
    "        # Convert labels to torch tensors\n",
    "        self.targets = torch.tensor(self.y_data, dtype=torch.long)\n",
    "\n",
    "        # Print the number of unique subjects (labels)\n",
    "        unique_subjects = len(np.unique(self.y_data))\n",
    "        print(f'Number of unique subjects: {unique_subjects}')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of data points.\n",
    "        \"\"\"\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single data point and its corresponding label and session.\n",
    "        Args:\n",
    "        - idx: Index of the data point to retrieve.\n",
    "        Returns:\n",
    "        - x_tensor: The EEG data as a torch tensor.\n",
    "        - y_tensor: The label as a torch tensor.\n",
    "        - session: The session identifier.\n",
    "        \"\"\"\n",
    "        # Efficiently fetch a single data point (use memory-mapped access for large datasets)\n",
    "        x_tensor = torch.tensor(self.x_data[idx], dtype=torch.float32)  # Convert to torch tensor\n",
    "        y_tensor = self.targets[idx]  # Fetch the preloaded label tensor\n",
    "        session = self.s_data[idx]  # Session info\n",
    "\n",
    "        return x_tensor, y_tensor, session\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close the HDF5 file when done.\n",
    "        \"\"\"\n",
    "        self.h5_file.close()\n",
    "\n",
    "# Usage example\n",
    "h5_file_path = 'train.h5'\n",
    "train_dataset = EEGDataset(h5_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae26074-7587-4026-bb04-58f8d2104e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matin\n"
     ]
    }
   ],
   "source": [
    "print(\"matin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88554489-03c5-47ea-8b9f-51e4f769e866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 489943\n",
      "Number of unique subjects: 292\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, h5_file_path):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with data from the specified HDF5 file.\n",
    "        Args:\n",
    "        - h5_file_path: Path to the HDF5 file containing the data and labels.\n",
    "        \"\"\"\n",
    "        # Open the HDF5 file\n",
    "        self.h5_file = h5py.File(h5_file_path, 'r')\n",
    "\n",
    "        # Access the data and labels from the HDF5 file\n",
    "        self.x_data = self.h5_file['data']  # EEG data (use memory-mapped access)\n",
    "        self.y_data = self.h5_file['labels'][:]  # Labels (load fully into memory)\n",
    "        self.s_data = self.h5_file['sessions'][:]  # Sessions (load fully into memory)\n",
    "\n",
    "        # Convert labels to torch tensors\n",
    "        self.targets = torch.tensor(self.y_data, dtype=torch.long)\n",
    "\n",
    "        # Print the number of samples and unique subjects (labels)\n",
    "        num_samples = self.x_data.shape[0]  # Number of samples is the first dimension of the data\n",
    "        unique_subjects = len(np.unique(self.y_data))  # Number of unique subjects\n",
    "        print(f'Number of samples: {num_samples}')\n",
    "        print(f'Number of unique subjects: {unique_subjects}')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of data points.\n",
    "        \"\"\"\n",
    "        return self.x_data.shape[0]  # The number of samples is the first dimension of x_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single data point and its corresponding label and session.\n",
    "        Args:\n",
    "        - idx: Index of the data point to retrieve.\n",
    "        Returns:\n",
    "        - x_tensor: The EEG data as a torch tensor.\n",
    "        - y_tensor: The label as a torch tensor.\n",
    "        - session: The session identifier.\n",
    "        \"\"\"\n",
    "        # Efficiently fetch a single data point (use memory-mapped access for large datasets)\n",
    "        x_tensor = torch.tensor(self.x_data[idx], dtype=torch.float32)  # Convert to torch tensor\n",
    "        y_tensor = self.targets[idx]  # Fetch the preloaded label tensor\n",
    "        session = self.s_data[idx]  # Session info\n",
    "\n",
    "        return x_tensor, y_tensor, session\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close the HDF5 file when done.\n",
    "        \"\"\"\n",
    "        self.h5_file.close()\n",
    "\n",
    "# Usage example\n",
    "h5_file_path = 'train.h5'\n",
    "train_dataset = EEGDataset(h5_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614fc28-ac71-4c36-ab48-2e1f1d7351d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
