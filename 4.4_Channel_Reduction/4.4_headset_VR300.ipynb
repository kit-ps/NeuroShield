{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a168ad0-9f2a-42a7-8d08-b0f34b939835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from sklearn.metrics.pairwise import cosine_distances as cd\n",
    "from collections import defaultdict\n",
    "from pyeer.eer_info import get_eer_stats\n",
    "from sklearn.metrics.pairwise import manhattan_distances as md\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "\n",
    "from pytorch_metric_learning.distances import LpDistance, CosineSimilarity,SNRDistance\n",
    "\n",
    "def EERf(resutls):\n",
    "    resutls= np.array(resutls)\n",
    "    genuine  = resutls[resutls[:, 1] == 1][:, 0]\n",
    "    impostor = resutls[resutls[:, 1] == 0][:, 0]\n",
    "    stats_a = get_eer_stats(genuine, impostor)\n",
    "    return(stats_a.eer,stats_a.fmr100)\n",
    "\n",
    "def calculate_and_print_averages(y_train, resutls3):\n",
    "    u, counts = np.unique(y_train, return_counts=True)\n",
    "    eer_values = []\n",
    "    ii = 0\n",
    "\n",
    "    for i in resutls3.keys():\n",
    "        re = EERf(resutls3[i])\n",
    "        eer = re[0]\n",
    "        print(f\"{i}: EER = {re[0]:.4f}, FMR100 = {re[1]:.4f}, Count = {counts[ii]}\")\n",
    "        eer_values.append(eer)\n",
    "        ii += 1\n",
    "\n",
    "    average_eer = np.mean(eer_values) * 100\n",
    "    std_eer = np.std(eer_values) * 100\n",
    "    \n",
    "    print(f\"Final Average EER: {average_eer:.4f}\")\n",
    "    print(f\"Final EER Standard Deviation: {std_eer:.4f}\")\n",
    "    print(f\"${average_eer:.2f} \\\\pm {std_eer:.2f}$\")\n",
    "    return(average_eer,std_eer)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_similarity_scores_two(enrollment_embeddings, y_enrollment, verification_embeddings, y_verification,distance):\n",
    "    similarity_results = []\n",
    "    similarity_results_by_class = []\n",
    "    similarity_results_by_class_dict = defaultdict(list)\n",
    "    unique_classes = np.unique(y_enrollment)\n",
    "    class_indices = [np.where(y_enrollment == cls)[0] for cls in unique_classes]\n",
    "\n",
    "    if distance == \"cd\":\n",
    "        similarity_matrix = -1 * cd(verification_embeddings, enrollment_embeddings)\n",
    "    elif distance == \"ed\":\n",
    "        similarity_matrix = -1 * ed(verification_embeddings, enrollment_embeddings)\n",
    "\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        current_class = y_verification[i]\n",
    "        predicted_scores = similarity_matrix[i]\n",
    "        same_class_indices = class_indices[np.where(unique_classes == current_class)[0][0]]\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            same_class_indices = class_indices[np.where(unique_classes == cls)[0][0]]\n",
    "            max_score = sum(sorted(predicted_scores[same_class_indices], reverse=True)[:10]) / 10\n",
    "            if current_class == cls:\n",
    "                similarity_results_by_class.append([max_score, 1, current_class, cls, i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 1, current_class, cls, i, cls])\n",
    "            else:\n",
    "                similarity_results_by_class.append([max_score, 0, current_class, cls, i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 0, current_class, cls, i, cls])\n",
    "\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict\n",
    "\n",
    "def assessment_model_data_two(enrollment_data, ye, verification_data, yv, e_network, distance):\n",
    "    x_enrollment, y_enrollment = enrollment_data, ye\n",
    "    x_verification, y_verification = verification_data, yv\n",
    "    enrollment_embeddings = compute_embedding_batch_two(x_enrollment, e_network)\n",
    "    verification_embeddings = compute_embedding_batch_two(x_verification, e_network)\n",
    "    similarity_results_by_class, similarity_results_by_class_dict = calculate_similarity_scores_two(\n",
    "        enrollment_embeddings, y_enrollment, verification_embeddings, y_verification,distance\n",
    "    )\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_embedding_batch_two(x_test_batch, embedding_network, batch_size=150, device=\"cuda\"):\n",
    "    total_samples = len(x_test_batch)\n",
    "    embeddings = []\n",
    "\n",
    "    # Process the data in batches\n",
    "    for start_index in range(0, total_samples, batch_size):\n",
    "        end_index = min(start_index + batch_size, total_samples)\n",
    "\n",
    "        batch = x_test_batch[start_index:end_index].copy()\n",
    "        # Convert the current batch to tensor and move to device\n",
    "        batch = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            batch_embeddings = embedding_network(batch).detach().cpu().numpy()\n",
    "\n",
    "        embeddings.append(batch_embeddings)  # Store embeddings on CPU\n",
    "\n",
    "        # Clear GPU memory for the batch\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all embeddings into a single numpy array\n",
    "    anchor_embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "    print(f\"Computed embeddings shape: {anchor_embeddings.shape}\")\n",
    "    return anchor_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3114c471-4001-4d09-be60-4c1e0fea6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EEGNet7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet7, self).__init__()\n",
    "        # InstanceNorm1d normalizes each channel across its 500 values for each sample\n",
    "        self.norm = nn.InstanceNorm1d(14, affine=False)  # Normalizes across the 500 values in each of the 93 channels\n",
    "        self.act = nn.ReLU()  # or nn.SELU() , ReLU\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.conv1 = nn.Conv2d(1, 256, (1, 4), padding='same')\n",
    "        self.pool1 = nn.MaxPool2d((1, 2))\n",
    "        self.conv2 = nn.Conv2d(256, 192, (4, 1), padding='same')\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))\n",
    "        self.conv3 = nn.Conv2d(192, 128, (1, 4), padding='same')\n",
    "        #self.pool3 = nn.MaxPool2d((1, 2))\n",
    "        self.conv4 = nn.Conv2d(128, 96, (4, 1), padding='same')\n",
    "        #self.pool4 = nn.MaxPool2d((2, 1))\n",
    "        self.conv5 = nn.Conv2d(96, 64, (1, 4), padding='same')\n",
    "        #self.pool5 = nn.MaxPool2d((1, 2))\n",
    "        self.conv6 = nn.Conv2d(64, 32, (4, 1), padding='same')\n",
    "        #self.pool6 = nn.MaxPool2d((2, 1))\n",
    "        self.conv7 = nn.Conv2d(32, 16, (1, 2), padding='same')\n",
    "        self.conv8 = nn.Conv2d(16, 2, (2, 1), padding='same')\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.embedding = nn.Linear(1500, 128)  # Embedding layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        #if x.dim() == 3:\n",
    "            #x = x.unsqueeze(1)  # Adding a channel dimension if input is 3D\n",
    "        x = self.norm(x)  # Apply InstanceNorm along the channel dimension (squeeze to 1D first)\n",
    "        x = x.unsqueeze(1)  # Reshape to 4D again for Conv2d\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.act(self.conv3(x))\n",
    "        #x = self.pool3(x)\n",
    "        x = self.act(self.conv4(x))\n",
    "        #x = self.pool4(x)\n",
    "        x = self.act(self.conv5(x))\n",
    "        #x = self.pool5(x)\n",
    "        x = self.act(self.conv6(x))\n",
    "        #x = self.pool6(x)\n",
    "        x = self.act(self.conv7(x))\n",
    "        x = self.act(self.conv8(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.embedding(x)\n",
    "        return x\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trunk = EEGNet7().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba46fff9-8dbc-40a0-a967-43ecb8430dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 70, Number of unique sessions: 19\n",
      "Subject 82, Number of unique sessions: 19\n",
      "Subject 85, Number of unique sessions: 19\n",
      "Subject 86, Number of unique sessions: 19\n",
      "Subject 96, Number of unique sessions: 18\n",
      "Subject 103, Number of unique sessions: 18\n",
      "Subject 106, Number of unique sessions: 41\n",
      "Subject 109, Number of unique sessions: 5\n",
      "Subject 111, Number of unique sessions: 19\n",
      "Subject 115, Number of unique sessions: 41\n",
      "Subject 118, Number of unique sessions: 19\n",
      "Subject 119, Number of unique sessions: 8\n",
      "Subject 125, Number of unique sessions: 19\n",
      "Subject 129, Number of unique sessions: 5\n",
      "Subject 130, Number of unique sessions: 19\n",
      "Subject 131, Number of unique sessions: 19\n",
      "Subject 136, Number of unique sessions: 19\n",
      "Subject 145, Number of unique sessions: 19\n",
      "Subject 156, Number of unique sessions: 6\n",
      "Subject 159, Number of unique sessions: 19\n",
      "Subject 167, Number of unique sessions: 2\n",
      "Subject 169, Number of unique sessions: 6\n",
      "Subject 173, Number of unique sessions: 6\n",
      "Subject 174, Number of unique sessions: 19\n",
      "Subject 175, Number of unique sessions: 6\n",
      "Subject 176, Number of unique sessions: 6\n",
      "Subject 181, Number of unique sessions: 5\n",
      "Subject 183, Number of unique sessions: 6\n",
      "Subject 184, Number of unique sessions: 19\n",
      "Subject 187, Number of unique sessions: 32\n",
      "Subject 191, Number of unique sessions: 19\n",
      "Subject 192, Number of unique sessions: 19\n",
      "Subject 194, Number of unique sessions: 19\n",
      "Subject 197, Number of unique sessions: 19\n",
      "Subject 199, Number of unique sessions: 19\n",
      "Subject 201, Number of unique sessions: 19\n",
      "Subject 203, Number of unique sessions: 6\n",
      "Subject 206, Number of unique sessions: 6\n",
      "Subject 207, Number of unique sessions: 41\n",
      "Subject 214, Number of unique sessions: 18\n",
      "Subject 216, Number of unique sessions: 6\n",
      "Subject 217, Number of unique sessions: 6\n",
      "Subject 219, Number of unique sessions: 5\n",
      "Subject 221, Number of unique sessions: 5\n",
      "Subject 225, Number of unique sessions: 6\n",
      "Subject 230, Number of unique sessions: 6\n",
      "Subject 231, Number of unique sessions: 19\n",
      "Subject 233, Number of unique sessions: 19\n",
      "Subject 236, Number of unique sessions: 42\n",
      "Subject 245, Number of unique sessions: 6\n",
      "Subject 249, Number of unique sessions: 42\n",
      "Subject 250, Number of unique sessions: 41\n",
      "Subject 251, Number of unique sessions: 41\n",
      "Subject 257, Number of unique sessions: 6\n",
      "Subject 260, Number of unique sessions: 49\n",
      "Subject 261, Number of unique sessions: 10\n",
      "Subject 262, Number of unique sessions: 6\n",
      "Subject 265, Number of unique sessions: 40\n",
      "Subject 268, Number of unique sessions: 19\n",
      "Subject 270, Number of unique sessions: 19\n",
      "Subject 277, Number of unique sessions: 13\n",
      "Subject 281, Number of unique sessions: 19\n",
      "Subject 282, Number of unique sessions: 5\n",
      "Subject 284, Number of unique sessions: 19\n",
      "Subject 286, Number of unique sessions: 19\n",
      "Subject 291, Number of unique sessions: 19\n",
      "Subject 296, Number of unique sessions: 23\n",
      "Subject 299, Number of unique sessions: 22\n",
      "Subject 301, Number of unique sessions: 23\n",
      "Subject 303, Number of unique sessions: 21\n",
      "Subject 306, Number of unique sessions: 23\n",
      "Subject 310, Number of unique sessions: 23\n",
      "Subject 314, Number of unique sessions: 6\n",
      "Subject 322, Number of unique sessions: 21\n",
      "Subject 325, Number of unique sessions: 22\n",
      "Subject 332, Number of unique sessions: 4\n",
      "Subject 336, Number of unique sessions: 23\n",
      "Subject 342, Number of unique sessions: 23\n",
      "Subject 348, Number of unique sessions: 23\n",
      "Subject 354, Number of unique sessions: 23\n",
      "Subject 357, Number of unique sessions: 22\n",
      "Subject 364, Number of unique sessions: 22\n",
      "Subject 365, Number of unique sessions: 30\n",
      "Subject 373, Number of unique sessions: 23\n",
      "Subject 374, Number of unique sessions: 20\n",
      "Subject 384, Number of unique sessions: 5\n",
      "Subject 385, Number of unique sessions: 22\n",
      "Subject 387, Number of unique sessions: 23\n",
      "Subject 402, Number of unique sessions: 9\n",
      "Subject 404, Number of unique sessions: 8\n",
      "Subject 407, Number of unique sessions: 9\n",
      "Subject 413, Number of unique sessions: 9\n",
      "Subject 414, Number of unique sessions: 9\n",
      "Subject 415, Number of unique sessions: 9\n",
      "Subject 418, Number of unique sessions: 9\n",
      "Subject 419, Number of unique sessions: 9\n",
      "Subject 422, Number of unique sessions: 8\n",
      "Subject 423, Number of unique sessions: 9\n",
      "Subject 433, Number of unique sessions: 7\n",
      "Subject 436, Number of unique sessions: 8\n",
      "x_test_e: (10000, 7, 500), y_test_e: (10000,), s_test_e: (10000,)\n",
      "x_test_v: (160051, 7, 500), y_test_v: (160051,), s_test_v: (160051,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2016154/921074870.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('./model/SupConLossLoss128_m4_e99_DSIVR300.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Load test data from test_raw.h5\n",
    "with h5py.File(\"./data/test_raw.h5\", \"r\") as f_test:\n",
    "    X_test = f_test['data'][:]\n",
    "    Y_test = f_test['labels'][:]\n",
    "    S_test = f_test['sessions'][:]\n",
    "    H_test = f_test['hardwares'][:]  # Load the 'hardwares' dataset as h_test\n",
    "\n",
    "# Load negative data from neg_raw.h5\n",
    "with h5py.File(\"./data/neg_raw.h5\", \"r\") as f_neg:\n",
    "    X_neg = f_neg['data'][:]\n",
    "    Y_neg = f_neg['labels'][:]\n",
    "    S_neg = f_neg['sessions'][:]\n",
    "    H_neg = f_neg['hardwares'][:]  # Load the 'hardwares' dataset as h_neg\n",
    "\n",
    "# Optional: Combine or use them independently depending on your requirement\n",
    "# Example of concatenating them for a combined dataset\n",
    "X_test = np.concatenate((X_test, X_neg), axis=0)\n",
    "Y_test = np.concatenate((Y_test, Y_neg), axis=0)\n",
    "S_test = np.concatenate((S_test, S_neg), axis=0)\n",
    "H_test = np.concatenate((H_test, H_neg), axis=0)\n",
    "\n",
    "headset_indices= [30, 62, 59, 63, 72, 78, 86]\n",
    "X_test = X_test[:, headset_indices, :]\n",
    "# Now, X_combined, Y_combined, S_combined, and h_combined contain the merged data\n",
    "\n",
    "\n",
    "# Now, X_combined, Y_combined, and S_combined contain the merged data\n",
    "\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Reorder the arrays according to the shuffled indices\n",
    "x_test = X_test[indices]\n",
    "y_test = Y_test[indices]\n",
    "s_test = S_test[indices]\n",
    "h_test = H_test[indices]\n",
    "\n",
    "\n",
    "# Find unique subjects\n",
    "unique_subjects = np.unique(y_test)\n",
    "\n",
    "# Initialize lists to hold the data for x_test_e and x_test_v\n",
    "x_test_e_list = []\n",
    "y_test_e_list = []\n",
    "s_test_e_list = []\n",
    "h_test_e_list = []\n",
    "\n",
    "x_test_v_list = []\n",
    "y_test_v_list = []\n",
    "s_test_v_list = []\n",
    "h_test_v_list = []\n",
    "\n",
    "# Assign the minimum session for each subject to x_test_e and the rest to x_test_v\n",
    "for subject in unique_subjects:\n",
    "    subject_indices = np.where(y_test == subject)[0]\n",
    "    subject_sessions = s_test[subject_indices]\n",
    "    \n",
    "    # Skip subjects with fewer than two unique sessions\n",
    "    if len(np.unique(subject_sessions)) < 2:\n",
    "        continue\n",
    "\n",
    "    print(f\"Subject {subject}, Number of unique sessions: {len(np.unique(subject_sessions))}\")\n",
    "    \n",
    "    # Assign the minimum session to the evaluation set (x_test_e)\n",
    "    min_session = np.min(subject_sessions)\n",
    "    \n",
    "    # Append data to the evaluation set (min session)\n",
    "    x_test_e_list.extend(x_test[subject_indices][subject_sessions == min_session])\n",
    "    y_test_e_list.extend(y_test[subject_indices][subject_sessions == min_session])\n",
    "    s_test_e_list.extend(s_test[subject_indices][subject_sessions == min_session])\n",
    "    h_test_e_list.extend(h_test[subject_indices][subject_sessions == min_session])\n",
    "\n",
    "    # Append remaining sessions to the validation set (x_test_v)\n",
    "    x_test_v_list.extend(x_test[subject_indices][subject_sessions != min_session])\n",
    "    y_test_v_list.extend(y_test[subject_indices][subject_sessions != min_session])\n",
    "    s_test_v_list.extend(s_test[subject_indices][subject_sessions != min_session])\n",
    "    h_test_v_list.extend(h_test[subject_indices][subject_sessions != min_session])\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_e\n",
    "indices_e = np.arange(len(x_test_e_list))\n",
    "np.random.shuffle(indices_e)\n",
    "\n",
    "x_test_e = np.array(x_test_e_list)[indices_e]\n",
    "y_test_e = np.array(y_test_e_list)[indices_e]\n",
    "s_test_e = np.array(s_test_e_list)[indices_e]\n",
    "h_test_e = np.array(h_test_v_list)[indices_e]\n",
    "\n",
    "# Shuffle and convert lists back to numpy arrays for x_test_v\n",
    "indices_v = np.arange(len(x_test_v_list))\n",
    "np.random.shuffle(indices_v)\n",
    "\n",
    "x_test_v = np.array(x_test_v_list)[indices_v]\n",
    "y_test_v = np.array(y_test_v_list)[indices_v]\n",
    "s_test_v = np.array(s_test_v_list)[indices_v]\n",
    "h_test_v = np.array(h_test_v_list)[indices_v]\n",
    "# Optional: Save the new test evaluation and validation sets to npy files (if needed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"x_test_e: {x_test_e.shape}, y_test_e: {y_test_e.shape}, s_test_e: {s_test_e.shape}\")\n",
    "print(f\"x_test_v: {x_test_v.shape}, y_test_v: {y_test_v.shape}, s_test_v: {s_test_v.shape}\")\n",
    "\n",
    "\n",
    "# Load the state dictionary from the saved file\n",
    "state_dict = torch.load('./model/SupConLossLoss128_m4_e99_DSIVR300.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "trunk.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7f72cd-e8ce-4331-8400-58138af5d168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fallahi/anaconda3/lib/python3.12/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n",
      "/home/fallahi/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:454: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1031.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings shape: (10000, 128)\n",
      "Computed embeddings shape: (160051, 128)\n"
     ]
    }
   ],
   "source": [
    "x_test_ee = compute_embedding_batch_two(x_test_e, trunk)\n",
    "x_test_ve = compute_embedding_batch_two(x_test_v, trunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96275cf2-74d0-4977-b2f0-eb157a4c54a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170051, 7, 500) (160051,) (10000,)\n",
      "70: EER = 0.2625, FMR100 = 0.9156, Count = 100\n",
      "82: EER = 0.1226, FMR100 = 0.6194, Count = 100\n",
      "85: EER = 0.1297, FMR100 = 0.6917, Count = 100\n",
      "86: EER = 0.0933, FMR100 = 0.5511, Count = 100\n",
      "96: EER = 0.1512, FMR100 = 0.7282, Count = 100\n",
      "103: EER = 0.0753, FMR100 = 0.4906, Count = 100\n",
      "106: EER = 0.3276, FMR100 = 0.8646, Count = 100\n",
      "109: EER = 0.3262, FMR100 = 0.9525, Count = 100\n",
      "111: EER = 0.2756, FMR100 = 0.9339, Count = 100\n",
      "115: EER = 0.3137, FMR100 = 0.8110, Count = 100\n",
      "118: EER = 0.1392, FMR100 = 0.7506, Count = 100\n",
      "119: EER = 0.1359, FMR100 = 0.7629, Count = 100\n",
      "125: EER = 0.1140, FMR100 = 0.7239, Count = 100\n",
      "129: EER = 0.3151, FMR100 = 0.9175, Count = 100\n",
      "130: EER = 0.1456, FMR100 = 0.6678, Count = 100\n",
      "131: EER = 0.1862, FMR100 = 0.8961, Count = 100\n",
      "136: EER = 0.1380, FMR100 = 0.7872, Count = 100\n",
      "145: EER = 0.1533, FMR100 = 0.6911, Count = 100\n",
      "156: EER = 0.0945, FMR100 = 0.5540, Count = 100\n",
      "159: EER = 0.1674, FMR100 = 0.8389, Count = 100\n",
      "167: EER = 0.1800, FMR100 = 0.9000, Count = 100\n",
      "169: EER = 0.2549, FMR100 = 0.8140, Count = 100\n",
      "173: EER = 0.2580, FMR100 = 0.9240, Count = 100\n",
      "174: EER = 0.2473, FMR100 = 0.9283, Count = 100\n",
      "175: EER = 0.1023, FMR100 = 0.4900, Count = 100\n",
      "176: EER = 0.0940, FMR100 = 0.4700, Count = 100\n",
      "181: EER = 0.1503, FMR100 = 0.9000, Count = 100\n",
      "183: EER = 0.0405, FMR100 = 0.2200, Count = 100\n",
      "184: EER = 0.0741, FMR100 = 0.3711, Count = 100\n",
      "187: EER = 0.2629, FMR100 = 0.8719, Count = 100\n",
      "191: EER = 0.2122, FMR100 = 0.8800, Count = 100\n",
      "192: EER = 0.2273, FMR100 = 0.8656, Count = 100\n",
      "194: EER = 0.2100, FMR100 = 0.8833, Count = 100\n",
      "197: EER = 0.1978, FMR100 = 0.8506, Count = 100\n",
      "199: EER = 0.1608, FMR100 = 0.7233, Count = 100\n",
      "201: EER = 0.1480, FMR100 = 0.7806, Count = 100\n",
      "203: EER = 0.0780, FMR100 = 0.4380, Count = 100\n",
      "206: EER = 0.0562, FMR100 = 0.4780, Count = 100\n",
      "207: EER = 0.2126, FMR100 = 0.8998, Count = 100\n",
      "214: EER = 0.1335, FMR100 = 0.6325, Count = 100\n",
      "216: EER = 0.1327, FMR100 = 0.5620, Count = 100\n",
      "217: EER = 0.0465, FMR100 = 0.2560, Count = 100\n",
      "219: EER = 0.1444, FMR100 = 0.6897, Count = 100\n",
      "221: EER = 0.1050, FMR100 = 0.8275, Count = 100\n",
      "225: EER = 0.1367, FMR100 = 0.8720, Count = 100\n",
      "230: EER = 0.2660, FMR100 = 0.9500, Count = 100\n",
      "231: EER = 0.1602, FMR100 = 0.7983, Count = 100\n",
      "233: EER = 0.2095, FMR100 = 0.8594, Count = 100\n",
      "236: EER = 0.3629, FMR100 = 0.9407, Count = 100\n",
      "245: EER = 0.0860, FMR100 = 0.5580, Count = 100\n",
      "249: EER = 0.2412, FMR100 = 0.7493, Count = 100\n",
      "250: EER = 0.3720, FMR100 = 0.9120, Count = 100\n",
      "251: EER = 0.2715, FMR100 = 0.8798, Count = 100\n",
      "257: EER = 0.1323, FMR100 = 0.8460, Count = 100\n",
      "260: EER = 0.2288, FMR100 = 0.7856, Count = 100\n",
      "261: EER = 0.1511, FMR100 = 0.8078, Count = 100\n",
      "262: EER = 0.0827, FMR100 = 0.4560, Count = 100\n",
      "265: EER = 0.2954, FMR100 = 0.9764, Count = 100\n",
      "268: EER = 0.2744, FMR100 = 0.9783, Count = 100\n",
      "270: EER = 0.2491, FMR100 = 0.8617, Count = 100\n",
      "277: EER = 0.1176, FMR100 = 0.6358, Count = 100\n",
      "281: EER = 0.1367, FMR100 = 0.6528, Count = 100\n",
      "282: EER = 0.2675, FMR100 = 0.9275, Count = 100\n",
      "284: EER = 0.1895, FMR100 = 0.7583, Count = 100\n",
      "286: EER = 0.1713, FMR100 = 0.7989, Count = 100\n",
      "291: EER = 0.2331, FMR100 = 0.9433, Count = 100\n",
      "296: EER = 0.2205, FMR100 = 0.8777, Count = 100\n",
      "299: EER = 0.1611, FMR100 = 0.7943, Count = 100\n",
      "301: EER = 0.1301, FMR100 = 0.7636, Count = 100\n",
      "303: EER = 0.1386, FMR100 = 0.7075, Count = 100\n",
      "306: EER = 0.2283, FMR100 = 0.9136, Count = 100\n",
      "310: EER = 0.1686, FMR100 = 0.8459, Count = 100\n",
      "314: EER = 0.1165, FMR100 = 0.7060, Count = 100\n",
      "322: EER = 0.1405, FMR100 = 0.8230, Count = 100\n",
      "325: EER = 0.1214, FMR100 = 0.6390, Count = 100\n",
      "332: EER = 0.2167, FMR100 = 0.3267, Count = 100\n",
      "336: EER = 0.2174, FMR100 = 0.9491, Count = 100\n",
      "342: EER = 0.1037, FMR100 = 0.6809, Count = 100\n",
      "348: EER = 0.1174, FMR100 = 0.7877, Count = 100\n",
      "354: EER = 0.0977, FMR100 = 0.5582, Count = 100\n",
      "357: EER = 0.1569, FMR100 = 0.8724, Count = 100\n",
      "364: EER = 0.0931, FMR100 = 0.5871, Count = 100\n",
      "365: EER = 0.1400, FMR100 = 0.8452, Count = 100\n",
      "373: EER = 0.0977, FMR100 = 0.5673, Count = 100\n",
      "374: EER = 0.0801, FMR100 = 0.6611, Count = 100\n",
      "384: EER = 0.1259, FMR100 = 0.7725, Count = 100\n",
      "385: EER = 0.1043, FMR100 = 0.6348, Count = 100\n",
      "387: EER = 0.1150, FMR100 = 0.7064, Count = 100\n",
      "402: EER = 0.1490, FMR100 = 0.8988, Count = 100\n",
      "404: EER = 0.1544, FMR100 = 0.8871, Count = 100\n",
      "407: EER = 0.1400, FMR100 = 0.8800, Count = 100\n",
      "413: EER = 0.1467, FMR100 = 0.7775, Count = 100\n",
      "414: EER = 0.0914, FMR100 = 0.5787, Count = 100\n",
      "415: EER = 0.1037, FMR100 = 0.6100, Count = 100\n",
      "418: EER = 0.1175, FMR100 = 0.8137, Count = 100\n",
      "419: EER = 0.1488, FMR100 = 0.8263, Count = 100\n",
      "422: EER = 0.1072, FMR100 = 0.7043, Count = 100\n",
      "423: EER = 0.0967, FMR100 = 0.6538, Count = 100\n",
      "433: EER = 0.0722, FMR100 = 0.5133, Count = 100\n",
      "436: EER = 0.0971, FMR100 = 0.5771, Count = 100\n",
      "Final Average EER: 16.5481\n",
      "Final EER Standard Deviation: 7.3369\n",
      "$16.55 \\pm 7.34$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16.548069643114346, 7.336914445939547)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HEREAA\n",
    "def compute_embedding_batch_two(x_test_batch, embedding_network, batch_size=100, device=\"cuda\"):\n",
    "    #print(x_test_batch.shape)\n",
    "    return x_test_batch\n",
    "\n",
    "\n",
    "\n",
    "# Generate a sequence of indices from 0 to the length of one of the arrays\n",
    "indices = np.arange(x_test.shape[0])\n",
    "\n",
    "\n",
    "print(x_test.shape,y_test_v.shape, y_test_e.shape)\n",
    "\n",
    "x_test_b = x_test_v\n",
    "y_test_b = y_test_v\n",
    "s_test_b = s_test_v\n",
    "\n",
    "\n",
    "#, s_test_b\n",
    "resutls2,resutls3=assessment_model_data_two(x_test_ee,y_test_e, x_test_ve,y_test_b,trunk, distance = \"cd\")\n",
    "#print(\"simple\",EERf(resutls))\n",
    "calculate_and_print_averages(y_test_e, resutls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca3016-b401-440d-8f20-71c227d9ac82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593548c-95dd-4498-9882-e7d10958c5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff5b8a-3fe5-414a-919b-0f888bc56f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7156ac39-5445-4590-b179-44b19149decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_scores_two2(enrollment_embeddings,y_enrollment, verification_embeddings, y_verification,s_ver, extra= 3):\n",
    "    flag_s=0\n",
    "    similarity_results = []\n",
    "    similarity_results_by_class = []\n",
    "    similarity_results_by_class_dict = defaultdict(list)\n",
    "    unique_classes = np.unique(y_enrollment)\n",
    "    print(unique_classes)\n",
    "    class_indices = [np.where(y_enrollment == cls)[0] for cls in unique_classes]\n",
    "    # Compute similarity matrix\n",
    "    print(\"I am 9\")\n",
    "    #similarity_matrix =batch_pairwise_distances(verification_embeddings, enrollment_embeddings, 100)\n",
    "\n",
    "    print(verification_embeddings.shape, enrollment_embeddings.shape )\n",
    "    \n",
    "    similarity_matrix = -1 * cd(verification_embeddings, enrollment_embeddings)\n",
    "    #similarity_matrix = abs(cs(verification_embeddings, enrollment_embeddings))\n",
    "    print(\"I am 10\")\n",
    "    print(similarity_matrix.shape)\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        current_class = y_verification[i]\n",
    "        subject_session = s_ver[i]\n",
    "        #current_ss= np.where((s_ver == subject_session) & (y_verification == current_class))\n",
    "        \n",
    "        current_ss = np.where((s_ver == subject_session) & (y_verification == current_class))[0]  # assuming s_ver == subject_session is an array of booleans\n",
    "        #print(\"aa\", len(current_ss))\n",
    "        \n",
    "        selected_indices = np.random.choice(current_ss, extra, replace=True)\n",
    "        selected_indices = np.append(selected_indices, i)\n",
    "\n",
    "        #print(current_class, y_verification[selected_indices], s_ver[selected_indices], selected_indices)\n",
    "\n",
    "        predicted_scoresg = similarity_matrix[selected_indices]\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        predicted_scores = similarity_matrix[i]\n",
    "\n",
    "\n",
    "        same_class_indices = class_indices[np.where(unique_classes == current_class)[0][0]]\n",
    "\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            same_class_indices = class_indices[np.where(unique_classes == cls)[0][0]]\n",
    "            #same_class_indices = np.setdiff1d(same_class_indices, [i])\n",
    "            #same_class_indices = np.setdiff1d(same_class_indices, [i])\n",
    "            #max_score = np.mean(predicted_scores[same_class_indices])\n",
    "            #max_score = max(predicted_scores[same_class_indices])\n",
    "            maxscore=[]\n",
    "            for ir in range(extra+1):\n",
    "                max_score = sum(sorted(predicted_scoresg[ir, same_class_indices], reverse=True)[:10]) / 10\n",
    "                maxscore.append(max_score)\n",
    "            max_score = sum(maxscore)/(extra+1)\n",
    "            \n",
    "            if current_class == cls:\n",
    "                similarity_results_by_class.append([max_score, 1, current_class, cls,i,i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 1, current_class, cls,i, cls ])\n",
    "            else:\n",
    "                similarity_results_by_class.append([max_score, 0, current_class, cls,i,i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 0, current_class, cls,i, cls])\n",
    "\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict\n",
    "\n",
    "def assessment_model_data_two2(enrollment_data, ye, verification_data,yv, e_network, s_ver, distance = \"cd\"):\n",
    "    x_enrollment, y_enrollment = enrollment_data , ye\n",
    "    x_verification, y_verification = verification_data, yv\n",
    "\n",
    "\n",
    "    print(f\"Total number of enrollment samples: {len(x_enrollment)}\")\n",
    "    print(f\"Total number of verification samples: {len(x_verification)}\", verification_data.shape)\n",
    "\n",
    "    # Compute embeddings for enrollment data\n",
    "    enrollment_embeddings = compute_embedding_batch_two(x_enrollment, e_network)\n",
    "    print(\"I am 0 - enrollment\", enrollment_embeddings.shape)\n",
    "    # Compute embeddings for verification data\n",
    "    verification_embeddings = compute_embedding_batch_two(x_verification, e_network)\n",
    "    print(\"I am 1\")\n",
    "\n",
    "    similarity_results_by_class, similarity_results_by_class_dict = calculate_similarity_scores_two2(\n",
    "        enrollment_embeddings,y_enrollment, verification_embeddings, y_verification, s_ver\n",
    "    )\n",
    "\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aecf5048-1016-49e6-b24c-7303dff11bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of enrollment samples: 10000\n",
      "Total number of verification samples: 160051 (160051, 128)\n",
      "I am 0 - enrollment (10000, 128)\n",
      "I am 1\n",
      "[ 70  82  85  86  96 103 106 109 111 115 118 119 125 129 130 131 136 145\n",
      " 156 159 167 169 173 174 175 176 181 183 184 187 191 192 194 197 199 201\n",
      " 203 206 207 214 216 217 219 221 225 230 231 233 236 245 249 250 251 257\n",
      " 260 261 262 265 268 270 277 281 282 284 286 291 296 299 301 303 306 310\n",
      " 314 322 325 332 336 342 348 354 357 364 365 373 374 384 385 387 402 404\n",
      " 407 413 414 415 418 419 422 423 433 436]\n",
      "I am 9\n",
      "(160051, 128) (10000, 128)\n",
      "I am 10\n",
      "(160051, 10000)\n",
      "70: EER = 0.2119, FMR100 = 0.8683, Count = 100\n",
      "82: EER = 0.0855, FMR100 = 0.3923, Count = 100\n",
      "85: EER = 0.0885, FMR100 = 0.5728, Count = 100\n",
      "86: EER = 0.0575, FMR100 = 0.3317, Count = 100\n",
      "96: EER = 0.0953, FMR100 = 0.5541, Count = 100\n",
      "103: EER = 0.0335, FMR100 = 0.1865, Count = 100\n",
      "106: EER = 0.3169, FMR100 = 0.8120, Count = 100\n",
      "109: EER = 0.3311, FMR100 = 0.9800, Count = 100\n",
      "111: EER = 0.2484, FMR100 = 0.9144, Count = 100\n",
      "115: EER = 0.2907, FMR100 = 0.7763, Count = 100\n",
      "118: EER = 0.0961, FMR100 = 0.6528, Count = 100\n",
      "119: EER = 0.0873, FMR100 = 0.6386, Count = 100\n",
      "125: EER = 0.0585, FMR100 = 0.3728, Count = 100\n",
      "129: EER = 0.3203, FMR100 = 0.9000, Count = 100\n",
      "130: EER = 0.1202, FMR100 = 0.4672, Count = 100\n",
      "131: EER = 0.1452, FMR100 = 0.8717, Count = 100\n",
      "136: EER = 0.0840, FMR100 = 0.6828, Count = 100\n",
      "145: EER = 0.1206, FMR100 = 0.5328, Count = 100\n",
      "156: EER = 0.0443, FMR100 = 0.2840, Count = 100\n",
      "159: EER = 0.1212, FMR100 = 0.7494, Count = 100\n",
      "167: EER = 0.0925, FMR100 = 0.9100, Count = 100\n",
      "169: EER = 0.2120, FMR100 = 0.6740, Count = 100\n",
      "173: EER = 0.2160, FMR100 = 0.9360, Count = 100\n",
      "174: EER = 0.2217, FMR100 = 0.9350, Count = 100\n",
      "175: EER = 0.0545, FMR100 = 0.2840, Count = 100\n",
      "176: EER = 0.0380, FMR100 = 0.1720, Count = 100\n",
      "181: EER = 0.0832, FMR100 = 0.8225, Count = 100\n",
      "183: EER = 0.0083, FMR100 = 0.0040, Count = 100\n",
      "184: EER = 0.0283, FMR100 = 0.1400, Count = 100\n",
      "187: EER = 0.2455, FMR100 = 0.7423, Count = 100\n",
      "191: EER = 0.1441, FMR100 = 0.7322, Count = 100\n",
      "192: EER = 0.1841, FMR100 = 0.8567, Count = 100\n",
      "194: EER = 0.1544, FMR100 = 0.7972, Count = 100\n",
      "197: EER = 0.1406, FMR100 = 0.7517, Count = 100\n",
      "199: EER = 0.0902, FMR100 = 0.5039, Count = 100\n",
      "201: EER = 0.1052, FMR100 = 0.6078, Count = 100\n",
      "203: EER = 0.0243, FMR100 = 0.1140, Count = 100\n",
      "206: EER = 0.0180, FMR100 = 0.0860, Count = 100\n",
      "207: EER = 0.1800, FMR100 = 0.9055, Count = 100\n",
      "214: EER = 0.1001, FMR100 = 0.3258, Count = 100\n",
      "216: EER = 0.0880, FMR100 = 0.4040, Count = 100\n",
      "217: EER = 0.0120, FMR100 = 0.0180, Count = 100\n",
      "219: EER = 0.0991, FMR100 = 0.4203, Count = 100\n",
      "221: EER = 0.0500, FMR100 = 0.7800, Count = 100\n",
      "225: EER = 0.0808, FMR100 = 0.7500, Count = 100\n",
      "230: EER = 0.2341, FMR100 = 0.9760, Count = 100\n",
      "231: EER = 0.1058, FMR100 = 0.7472, Count = 100\n",
      "233: EER = 0.1462, FMR100 = 0.8172, Count = 100\n",
      "236: EER = 0.3643, FMR100 = 0.9300, Count = 100\n",
      "245: EER = 0.0380, FMR100 = 0.2000, Count = 100\n",
      "249: EER = 0.2120, FMR100 = 0.6476, Count = 100\n",
      "250: EER = 0.3907, FMR100 = 0.8860, Count = 100\n",
      "251: EER = 0.2369, FMR100 = 0.8223, Count = 100\n",
      "257: EER = 0.0709, FMR100 = 0.6220, Count = 100\n",
      "260: EER = 0.2087, FMR100 = 0.6288, Count = 100\n",
      "261: EER = 0.0967, FMR100 = 0.6444, Count = 100\n",
      "262: EER = 0.0380, FMR100 = 0.1580, Count = 100\n",
      "265: EER = 0.2669, FMR100 = 0.9851, Count = 100\n",
      "268: EER = 0.2213, FMR100 = 0.9911, Count = 100\n",
      "270: EER = 0.2256, FMR100 = 0.7583, Count = 100\n",
      "277: EER = 0.0545, FMR100 = 0.3200, Count = 100\n",
      "281: EER = 0.0746, FMR100 = 0.3950, Count = 100\n",
      "282: EER = 0.2037, FMR100 = 0.9325, Count = 100\n",
      "284: EER = 0.1444, FMR100 = 0.6283, Count = 100\n",
      "286: EER = 0.1019, FMR100 = 0.5306, Count = 100\n",
      "291: EER = 0.1891, FMR100 = 0.9533, Count = 100\n",
      "296: EER = 0.1820, FMR100 = 0.8673, Count = 100\n",
      "299: EER = 0.1048, FMR100 = 0.5595, Count = 100\n",
      "301: EER = 0.0693, FMR100 = 0.5591, Count = 100\n",
      "303: EER = 0.0682, FMR100 = 0.4495, Count = 100\n",
      "306: EER = 0.1909, FMR100 = 0.9218, Count = 100\n",
      "310: EER = 0.1324, FMR100 = 0.8500, Count = 100\n",
      "314: EER = 0.0809, FMR100 = 0.5820, Count = 100\n",
      "322: EER = 0.0935, FMR100 = 0.7335, Count = 100\n",
      "325: EER = 0.0852, FMR100 = 0.4105, Count = 100\n",
      "332: EER = 0.2200, FMR100 = 0.3333, Count = 100\n",
      "336: EER = 0.1878, FMR100 = 0.9536, Count = 100\n",
      "342: EER = 0.0709, FMR100 = 0.4595, Count = 100\n",
      "348: EER = 0.0969, FMR100 = 0.6968, Count = 100\n",
      "354: EER = 0.0659, FMR100 = 0.2664, Count = 100\n",
      "357: EER = 0.1481, FMR100 = 0.8105, Count = 100\n",
      "364: EER = 0.0590, FMR100 = 0.3386, Count = 100\n",
      "365: EER = 0.1253, FMR100 = 0.7483, Count = 100\n",
      "373: EER = 0.0560, FMR100 = 0.2577, Count = 100\n",
      "374: EER = 0.0507, FMR100 = 0.3600, Count = 100\n",
      "384: EER = 0.0975, FMR100 = 0.6650, Count = 100\n",
      "385: EER = 0.0672, FMR100 = 0.4757, Count = 100\n",
      "387: EER = 0.0868, FMR100 = 0.5250, Count = 100\n",
      "402: EER = 0.1278, FMR100 = 0.8425, Count = 100\n",
      "404: EER = 0.1286, FMR100 = 0.9029, Count = 100\n",
      "407: EER = 0.1140, FMR100 = 0.8912, Count = 100\n",
      "413: EER = 0.1467, FMR100 = 0.6538, Count = 100\n",
      "414: EER = 0.0601, FMR100 = 0.3725, Count = 100\n",
      "415: EER = 0.0812, FMR100 = 0.5537, Count = 100\n",
      "418: EER = 0.0844, FMR100 = 0.7238, Count = 100\n",
      "419: EER = 0.1450, FMR100 = 0.7675, Count = 100\n",
      "422: EER = 0.0774, FMR100 = 0.5214, Count = 100\n",
      "423: EER = 0.0451, FMR100 = 0.3137, Count = 100\n",
      "433: EER = 0.0339, FMR100 = 0.2167, Count = 100\n",
      "436: EER = 0.0671, FMR100 = 0.3029, Count = 100\n",
      "Final Average EER: 12.7031\n",
      "Final EER Standard Deviation: 8.1559\n",
      "$12.70 \\pm 8.16$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12.703086967759905, 8.1558837504068)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#, s_test_b\n",
    "resutls2,resutls3=assessment_model_data_two2(x_test_ee,y_test_e, x_test_ve,y_test_v,trunk, s_test_v, distance = \"cd\")\n",
    "#print(\"simple\",EERf(resutls))\n",
    "calculate_and_print_averages(y_test_e, resutls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8782e9-d0fa-4b6b-9ef3-03265bbf1e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821ba2d-5032-4437-86d3-6a1d0f246d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc59c1-0204-44dc-a340-9e3bf2fc1676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8623bb6-52ab-41d4-bbf6-289820532995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0efe077-b882-4fc4-b017-3bbd01421a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test = np.concatenate((x_test_ee, x_test_ve), axis=0)\n",
    "Y_test = np.concatenate((y_test_e, y_test_v), axis=0)\n",
    "S_test = np.concatenate((s_test_e, s_test_v), axis=0)\n",
    "H_test = np.concatenate((h_test_e, h_test_v), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a617bc76-2229-43c7-9b8e-8b35fa6e66d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects: 99\n",
      "x_test_e: (19800, 128), y_test_e: (19800,), s_test_e: (19800,)\n",
      "x_test_v: (9900, 128), y_test_v: (9900,), s_test_v: (9900,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#uy= np.unique(Y_test)[:5]\n",
    "#ynl = np.isin(Y_test,uy)\n",
    "\n",
    "\n",
    "#X_test = X_test[ynl]\n",
    "#Y_test = Y_test[ynl]\n",
    "#S_test = S_test[ynl]\n",
    "\n",
    "# Generate a sequence of indices from 0 to the length of one of the arrays\n",
    "indices = np.arange(X_test.shape[0])\n",
    "\n",
    "# Shuffle the indices\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Use the shuffled indices to reorder the arrays\n",
    "x_test = X_test[indices]\n",
    "y_test = Y_test[indices]\n",
    "s_test = S_test[indices]\n",
    "\n",
    "# Find unique subjects\n",
    "unique_subjects = np.unique(y_test)\n",
    "\n",
    "# Initialize lists to hold the data for x_test_e and x_test_v\n",
    "x_test_e_list = []\n",
    "y_test_e_list = []\n",
    "s_test_e_list = []\n",
    "\n",
    "x_test_v_list = []\n",
    "y_test_v_list = []\n",
    "s_test_v_list = []\n",
    "\n",
    "# Assign the minimum session for each subject to x_test_e and the rest to x_test_v\n",
    "# Define the number of sessions for enrollment and verification\n",
    "num_enrollment_sessions = 2\n",
    "num_verification_sessions = 1  # This can be adjusted as needed\n",
    "\n",
    "for subject in unique_subjects:\n",
    "    subject_indices = np.where(y_test == subject)[0]\n",
    "    subject_sessions = s_test[subject_indices]\n",
    "    unique_sessions = np.unique(subject_sessions)\n",
    "    \n",
    "    # Ensure there are enough sessions available for both enrollment and verification\n",
    "    if len(unique_sessions) < (num_enrollment_sessions + num_verification_sessions):\n",
    "        continue\n",
    "\n",
    "    #print(subject, len(unique_sessions))\n",
    "    \n",
    "    # Select sessions for enrollment and verification\n",
    "    enrollment_sessions = unique_sessions[:num_enrollment_sessions]\n",
    "    verification_sessions = unique_sessions[num_enrollment_sessions:num_enrollment_sessions + num_verification_sessions]\n",
    "    \n",
    "    # Extend the enrollment lists with data from selected sessions\n",
    "    x_test_e_list.extend(x_test[subject_indices][np.isin(subject_sessions, enrollment_sessions)])\n",
    "    y_test_e_list.extend(y_test[subject_indices][np.isin(subject_sessions, enrollment_sessions)])\n",
    "    s_test_e_list.extend(s_test[subject_indices][np.isin(subject_sessions, enrollment_sessions)])\n",
    "\n",
    "    # Extend the verification lists with data from selected sessions\n",
    "    x_test_v_list.extend(x_test[subject_indices][np.isin(subject_sessions, verification_sessions)])\n",
    "    y_test_v_list.extend(y_test[subject_indices][np.isin(subject_sessions, verification_sessions)])\n",
    "    s_test_v_list.extend(s_test[subject_indices][np.isin(subject_sessions, verification_sessions)])\n",
    "\n",
    "\n",
    "indices = np.arange(np.array(x_test_e_list).shape[0])\n",
    "\n",
    "# Shuffle the indices\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Use the shuffled indices to reorder the arrays\n",
    "\n",
    "\n",
    "# Convert lists back to numpy arrays\n",
    "x_test_e = np.array(x_test_e_list)\n",
    "y_test_e = np.array(y_test_e_list)\n",
    "s_test_e = np.array(s_test_e_list)\n",
    "\n",
    "x_test_e = x_test_e[indices]\n",
    "y_test_e = y_test_e[indices]\n",
    "s_test_e = s_test_e[indices]\n",
    "\n",
    "\n",
    "indices = np.arange(np.array(x_test_v_list).shape[0])\n",
    "\n",
    "# Shuffle the indices\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "\n",
    "x_test_v = np.array(x_test_v_list)\n",
    "y_test_v = np.array(y_test_v_list)\n",
    "s_test_v = np.array(s_test_v_list)\n",
    "\n",
    "# Use the shuffled indices to reorder the arrays\n",
    "x_test_v = x_test_v[indices]\n",
    "y_test_v = y_test_v[indices]\n",
    "s_test_v = s_test_v[indices]\n",
    "\n",
    "print(\"subjects:\", len(np.unique(y_test_e)))\n",
    "print(f\"x_test_e: {x_test_e.shape}, y_test_e: {y_test_e.shape}, s_test_e: {s_test_e.shape}\")\n",
    "print(f\"x_test_v: {x_test_v.shape}, y_test_v: {y_test_v.shape}, s_test_v: {s_test_v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "babb342f-927a-465f-b0cf-14ac2c61cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_scores_two2(enrollment_embeddings,y_enrollment, verification_embeddings, y_verification,s_ver, extra= 0):\n",
    "    flag_s=0\n",
    "    similarity_results = []\n",
    "    similarity_results_by_class = []\n",
    "    similarity_results_by_class_dict = defaultdict(list)\n",
    "    unique_classes = np.unique(y_enrollment)\n",
    "    print(unique_classes)\n",
    "    class_indices = [np.where(y_enrollment == cls)[0] for cls in unique_classes]\n",
    "    # Compute similarity matrix\n",
    "    print(\"I am 9\")\n",
    "    #similarity_matrix =batch_pairwise_distances(verification_embeddings, enrollment_embeddings, 100)\n",
    "\n",
    "    print(verification_embeddings.shape, enrollment_embeddings.shape )\n",
    "    \n",
    "    similarity_matrix = -1 * cd(verification_embeddings, enrollment_embeddings)\n",
    "    #similarity_matrix = abs(cs(verification_embeddings, enrollment_embeddings))\n",
    "    print(\"I am 10\")\n",
    "    print(similarity_matrix.shape)\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        current_class = y_verification[i]\n",
    "        subject_session = s_ver[i]\n",
    "        #current_ss= np.where((s_ver == subject_session) & (y_verification == current_class))\n",
    "        \n",
    "        current_ss = np.where((s_ver == subject_session) & (y_verification == current_class))[0]  # assuming s_ver == subject_session is an array of booleans\n",
    "        #print(\"aa\", len(current_ss))\n",
    "        \n",
    "        selected_indices = np.random.choice(current_ss, extra, replace=True)\n",
    "        selected_indices = np.append(selected_indices, i)\n",
    "\n",
    "        #print(current_class, y_verification[selected_indices], s_ver[selected_indices], selected_indices)\n",
    "\n",
    "        predicted_scoresg = similarity_matrix[selected_indices]\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        predicted_scores = similarity_matrix[i]\n",
    "\n",
    "\n",
    "        same_class_indices = class_indices[np.where(unique_classes == current_class)[0][0]]\n",
    "\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            same_class_indices = class_indices[np.where(unique_classes == cls)[0][0]]\n",
    "            #same_class_indices = np.setdiff1d(same_class_indices, [i])\n",
    "            #same_class_indices = np.setdiff1d(same_class_indices, [i])\n",
    "            #max_score = np.mean(predicted_scores[same_class_indices])\n",
    "            #max_score = max(predicted_scores[same_class_indices])\n",
    "            maxscore=[]\n",
    "            for ir in range(extra+1):\n",
    "                max_score = sum(sorted(predicted_scoresg[ir, same_class_indices], reverse=True)[:10]) / 10\n",
    "                maxscore.append(max_score)\n",
    "            max_score = sum(maxscore)/(extra+1)\n",
    "            \n",
    "            if current_class == cls:\n",
    "                similarity_results_by_class.append([max_score, 1, current_class, cls,i,i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 1, current_class, cls,i, cls ])\n",
    "            else:\n",
    "                similarity_results_by_class.append([max_score, 0, current_class, cls,i,i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 0, current_class, cls,i, cls])\n",
    "\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict\n",
    "\n",
    "def assessment_model_data_two2(enrollment_data, ye, verification_data,yv, e_network, s_ver, distance = \"cd\"):\n",
    "    x_enrollment, y_enrollment = enrollment_data , ye\n",
    "    x_verification, y_verification = verification_data, yv\n",
    "\n",
    "\n",
    "    print(f\"Total number of enrollment samples: {len(x_enrollment)}\")\n",
    "    print(f\"Total number of verification samples: {len(x_verification)}\", verification_data.shape)\n",
    "\n",
    "    # Compute embeddings for enrollment data\n",
    "    enrollment_embeddings = compute_embedding_batch_two(x_enrollment, e_network)\n",
    "    print(\"I am 0 - enrollment\", enrollment_embeddings.shape)\n",
    "    # Compute embeddings for verification data\n",
    "    verification_embeddings = compute_embedding_batch_two(x_verification, e_network)\n",
    "    print(\"I am 1\")\n",
    "\n",
    "    similarity_results_by_class, similarity_results_by_class_dict = calculate_similarity_scores_two2(\n",
    "        enrollment_embeddings,y_enrollment, verification_embeddings, y_verification, s_ver\n",
    "    )\n",
    "\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c2f7f53-760c-492a-aa39-013531b29706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of enrollment samples: 19800\n",
      "Total number of verification samples: 9900 (9900, 128)\n",
      "I am 0 - enrollment (19800, 128)\n",
      "I am 1\n",
      "[ 70  82  85  86  96 103 106 109 111 115 118 119 125 129 130 131 136 145\n",
      " 156 159 169 173 174 175 176 181 183 184 187 191 192 194 197 199 201 203\n",
      " 206 207 214 216 217 219 221 225 230 231 233 236 245 249 250 251 257 260\n",
      " 261 262 265 268 270 277 281 282 284 286 291 296 299 301 303 306 310 314\n",
      " 322 325 332 336 342 348 354 357 364 365 373 374 384 385 387 402 404 407\n",
      " 413 414 415 418 419 422 423 433 436]\n",
      "I am 9\n",
      "(9900, 128) (19800, 128)\n",
      "I am 10\n",
      "(9900, 19800)\n",
      "70: EER = 0.1230, FMR100 = 0.7400, Count = 200\n",
      "82: EER = 0.0321, FMR100 = 0.3300, Count = 200\n",
      "85: EER = 0.0700, FMR100 = 0.7100, Count = 200\n",
      "86: EER = 0.0149, FMR100 = 0.0800, Count = 200\n",
      "96: EER = 0.1000, FMR100 = 0.6800, Count = 200\n",
      "103: EER = 0.0416, FMR100 = 0.4100, Count = 200\n",
      "106: EER = 0.1107, FMR100 = 0.8100, Count = 200\n",
      "109: EER = 0.0900, FMR100 = 0.3500, Count = 200\n",
      "111: EER = 0.2300, FMR100 = 0.7500, Count = 200\n",
      "115: EER = 0.2312, FMR100 = 1.0000, Count = 200\n",
      "118: EER = 0.0735, FMR100 = 0.6100, Count = 200\n",
      "119: EER = 0.1820, FMR100 = 1.0000, Count = 200\n",
      "125: EER = 0.2700, FMR100 = 0.5900, Count = 200\n",
      "129: EER = 0.2103, FMR100 = 0.7100, Count = 200\n",
      "130: EER = 0.0413, FMR100 = 0.0800, Count = 200\n",
      "131: EER = 0.3400, FMR100 = 1.0000, Count = 200\n",
      "136: EER = 0.1122, FMR100 = 0.9700, Count = 200\n",
      "145: EER = 0.2409, FMR100 = 0.9300, Count = 200\n",
      "156: EER = 0.1028, FMR100 = 0.8100, Count = 200\n",
      "159: EER = 0.1200, FMR100 = 0.6600, Count = 200\n",
      "169: EER = 0.3600, FMR100 = 0.9800, Count = 200\n",
      "173: EER = 0.1131, FMR100 = 0.8800, Count = 200\n",
      "174: EER = 0.1048, FMR100 = 0.8500, Count = 200\n",
      "175: EER = 0.0535, FMR100 = 0.3000, Count = 200\n",
      "176: EER = 0.0506, FMR100 = 0.2800, Count = 200\n",
      "181: EER = 0.1700, FMR100 = 0.8300, Count = 200\n",
      "183: EER = 0.0447, FMR100 = 0.3900, Count = 200\n",
      "184: EER = 0.0700, FMR100 = 0.4000, Count = 200\n",
      "187: EER = 0.1300, FMR100 = 0.6700, Count = 200\n",
      "191: EER = 0.1612, FMR100 = 0.8400, Count = 200\n",
      "192: EER = 0.1203, FMR100 = 0.8200, Count = 200\n",
      "194: EER = 0.1419, FMR100 = 0.8100, Count = 200\n",
      "197: EER = 0.3629, FMR100 = 0.9800, Count = 200\n",
      "199: EER = 0.0819, FMR100 = 0.4500, Count = 200\n",
      "201: EER = 0.1218, FMR100 = 0.8100, Count = 200\n",
      "203: EER = 0.0700, FMR100 = 0.4300, Count = 200\n",
      "206: EER = 0.0504, FMR100 = 0.4300, Count = 200\n",
      "207: EER = 0.1919, FMR100 = 0.9900, Count = 200\n",
      "214: EER = 0.0501, FMR100 = 0.4400, Count = 200\n",
      "216: EER = 0.0916, FMR100 = 0.5600, Count = 200\n",
      "217: EER = 0.0300, FMR100 = 0.2800, Count = 200\n",
      "219: EER = 0.0731, FMR100 = 0.3400, Count = 200\n",
      "221: EER = 0.0708, FMR100 = 0.6900, Count = 200\n",
      "225: EER = 0.1214, FMR100 = 0.8400, Count = 200\n",
      "230: EER = 0.1221, FMR100 = 0.7700, Count = 200\n",
      "231: EER = 0.1135, FMR100 = 0.4900, Count = 200\n",
      "233: EER = 0.1800, FMR100 = 0.9800, Count = 200\n",
      "236: EER = 0.1400, FMR100 = 0.7400, Count = 200\n",
      "245: EER = 0.1000, FMR100 = 0.6200, Count = 200\n",
      "249: EER = 0.2900, FMR100 = 0.9700, Count = 200\n",
      "250: EER = 0.1000, FMR100 = 0.4800, Count = 200\n",
      "251: EER = 0.0838, FMR100 = 0.6700, Count = 200\n",
      "257: EER = 0.0820, FMR100 = 0.7200, Count = 200\n",
      "260: EER = 0.0814, FMR100 = 0.5800, Count = 200\n",
      "261: EER = 0.1548, FMR100 = 0.7900, Count = 200\n",
      "262: EER = 0.0631, FMR100 = 0.3000, Count = 200\n",
      "265: EER = 0.2043, FMR100 = 0.9900, Count = 200\n",
      "268: EER = 0.1749, FMR100 = 0.9100, Count = 200\n",
      "270: EER = 0.1822, FMR100 = 0.7400, Count = 200\n",
      "277: EER = 0.0928, FMR100 = 0.6400, Count = 200\n",
      "281: EER = 0.0839, FMR100 = 0.6300, Count = 200\n",
      "282: EER = 0.1100, FMR100 = 0.7800, Count = 200\n",
      "284: EER = 0.0927, FMR100 = 0.6400, Count = 200\n",
      "286: EER = 0.1448, FMR100 = 0.7900, Count = 200\n",
      "291: EER = 0.2639, FMR100 = 0.9800, Count = 200\n",
      "296: EER = 0.1733, FMR100 = 0.9600, Count = 200\n",
      "299: EER = 0.0800, FMR100 = 0.5700, Count = 200\n",
      "301: EER = 0.0818, FMR100 = 0.7200, Count = 200\n",
      "303: EER = 0.0535, FMR100 = 0.3500, Count = 200\n",
      "306: EER = 0.0721, FMR100 = 0.5600, Count = 200\n",
      "310: EER = 0.0700, FMR100 = 0.8400, Count = 200\n",
      "314: EER = 0.1300, FMR100 = 0.6000, Count = 200\n",
      "322: EER = 0.0943, FMR100 = 0.4700, Count = 200\n",
      "325: EER = 0.1528, FMR100 = 0.9300, Count = 200\n",
      "332: EER = 0.0400, FMR100 = 0.0900, Count = 200\n",
      "336: EER = 0.1834, FMR100 = 1.0000, Count = 200\n",
      "342: EER = 0.1000, FMR100 = 0.7400, Count = 200\n",
      "348: EER = 0.0818, FMR100 = 0.7100, Count = 200\n",
      "354: EER = 0.0900, FMR100 = 0.4000, Count = 200\n",
      "357: EER = 0.2006, FMR100 = 0.9900, Count = 200\n",
      "364: EER = 0.0743, FMR100 = 0.6300, Count = 200\n",
      "365: EER = 0.0834, FMR100 = 0.5800, Count = 200\n",
      "373: EER = 0.1418, FMR100 = 0.7900, Count = 200\n",
      "374: EER = 0.0637, FMR100 = 0.5500, Count = 200\n",
      "384: EER = 0.0739, FMR100 = 0.5300, Count = 200\n",
      "385: EER = 0.0600, FMR100 = 0.4000, Count = 200\n",
      "387: EER = 0.0349, FMR100 = 0.2800, Count = 200\n",
      "402: EER = 0.1700, FMR100 = 0.9400, Count = 200\n",
      "404: EER = 0.1300, FMR100 = 0.7000, Count = 200\n",
      "407: EER = 0.1730, FMR100 = 0.9300, Count = 200\n",
      "413: EER = 0.1210, FMR100 = 0.7700, Count = 200\n",
      "414: EER = 0.0618, FMR100 = 0.5600, Count = 200\n",
      "415: EER = 0.1034, FMR100 = 0.7300, Count = 200\n",
      "418: EER = 0.1327, FMR100 = 0.9000, Count = 200\n",
      "419: EER = 0.1002, FMR100 = 0.5800, Count = 200\n",
      "422: EER = 0.0709, FMR100 = 0.5100, Count = 200\n",
      "423: EER = 0.0928, FMR100 = 0.7500, Count = 200\n",
      "433: EER = 0.0345, FMR100 = 0.3200, Count = 200\n",
      "436: EER = 0.0647, FMR100 = 0.4900, Count = 200\n",
      "Final Average EER: 11.9429\n",
      "Final EER Standard Deviation: 7.0447\n",
      "$11.94 \\pm 7.04$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11.942949907235622, 7.044733163868219)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#, s_test_b\n",
    "resutls2,resutls3=assessment_model_data_two2(x_test_e,y_test_e, x_test_v,y_test_v,None, s_test_v, distance = \"cd\")\n",
    "#print(\"simple\",EERf(resutls))\n",
    "calculate_and_print_averages(y_test_e, resutls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49636bb0-fa01-490a-9431-d9ebcc26a96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b22be-c17a-4f78-a013-22fb86214abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62c22f41-7689-41ee-851f-35d10bd74775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_scores_two2(enrollment_embeddings,y_enrollment, verification_embeddings, y_verification,s_ver, extra= 3):\n",
    "    flag_s=0\n",
    "    similarity_results = []\n",
    "    similarity_results_by_class = []\n",
    "    similarity_results_by_class_dict = defaultdict(list)\n",
    "    unique_classes = np.unique(y_enrollment)\n",
    "    print(unique_classes)\n",
    "    class_indices = [np.where(y_enrollment == cls)[0] for cls in unique_classes]\n",
    "    # Compute similarity matrix\n",
    "    print(\"I am 9\")\n",
    "    #similarity_matrix =batch_pairwise_distances(verification_embeddings, enrollment_embeddings, 100)\n",
    "\n",
    "    print(verification_embeddings.shape, enrollment_embeddings.shape )\n",
    "    \n",
    "    similarity_matrix = -1 * cd(verification_embeddings, enrollment_embeddings)\n",
    "    #similarity_matrix = abs(cs(verification_embeddings, enrollment_embeddings))\n",
    "    print(\"I am 10\")\n",
    "    print(similarity_matrix.shape)\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        current_class = y_verification[i]\n",
    "        subject_session = s_ver[i]\n",
    "        #current_ss= np.where((s_ver == subject_session) & (y_verification == current_class))\n",
    "        \n",
    "        current_ss = np.where((s_ver == subject_session) & (y_verification == current_class))[0]  # assuming s_ver == subject_session is an array of booleans\n",
    "        #print(\"aa\", len(current_ss))\n",
    "        \n",
    "        selected_indices = np.random.choice(current_ss, extra, replace=True)\n",
    "        selected_indices = np.append(selected_indices, i)\n",
    "\n",
    "        #print(current_class, y_verification[selected_indices], s_ver[selected_indices], selected_indices)\n",
    "\n",
    "        predicted_scoresg = similarity_matrix[selected_indices]\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        predicted_scores = similarity_matrix[i]\n",
    "\n",
    "\n",
    "        same_class_indices = class_indices[np.where(unique_classes == current_class)[0][0]]\n",
    "\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            same_class_indices = class_indices[np.where(unique_classes == cls)[0][0]]\n",
    "            #same_class_indices = np.setdiff1d(same_class_indices, [i])\n",
    "            #same_class_indices = np.setdiff1d(same_class_indices, [i])\n",
    "            #max_score = np.mean(predicted_scores[same_class_indices])\n",
    "            #max_score = max(predicted_scores[same_class_indices])\n",
    "            maxscore=[]\n",
    "            for ir in range(extra+1):\n",
    "                max_score = sum(sorted(predicted_scoresg[ir, same_class_indices], reverse=True)[:10]) / 10\n",
    "                maxscore.append(max_score)\n",
    "            max_score = sum(maxscore)/(extra+1)\n",
    "            \n",
    "            if current_class == cls:\n",
    "                similarity_results_by_class.append([max_score, 1, current_class, cls,i,i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 1, current_class, cls,i, cls ])\n",
    "            else:\n",
    "                similarity_results_by_class.append([max_score, 0, current_class, cls,i,i])\n",
    "                similarity_results_by_class_dict[cls].append([max_score, 0, current_class, cls,i, cls])\n",
    "\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict\n",
    "\n",
    "def assessment_model_data_two2(enrollment_data, ye, verification_data,yv, e_network, s_ver, distance = \"cd\"):\n",
    "    x_enrollment, y_enrollment = enrollment_data , ye\n",
    "    x_verification, y_verification = verification_data, yv\n",
    "\n",
    "\n",
    "    print(f\"Total number of enrollment samples: {len(x_enrollment)}\")\n",
    "    print(f\"Total number of verification samples: {len(x_verification)}\", verification_data.shape)\n",
    "\n",
    "    # Compute embeddings for enrollment data\n",
    "    enrollment_embeddings = compute_embedding_batch_two(x_enrollment, e_network)\n",
    "    print(\"I am 0 - enrollment\", enrollment_embeddings.shape)\n",
    "    # Compute embeddings for verification data\n",
    "    verification_embeddings = compute_embedding_batch_two(x_verification, e_network)\n",
    "    print(\"I am 1\")\n",
    "\n",
    "    similarity_results_by_class, similarity_results_by_class_dict = calculate_similarity_scores_two2(\n",
    "        enrollment_embeddings,y_enrollment, verification_embeddings, y_verification, s_ver\n",
    "    )\n",
    "\n",
    "    return similarity_results_by_class, similarity_results_by_class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d869b652-38ba-4c86-9a00-03bfc2627878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of enrollment samples: 19800\n",
      "Total number of verification samples: 9900 (9900, 128)\n",
      "I am 0 - enrollment (19800, 128)\n",
      "I am 1\n",
      "[ 70  82  85  86  96 103 106 109 111 115 118 119 125 129 130 131 136 145\n",
      " 156 159 169 173 174 175 176 181 183 184 187 191 192 194 197 199 201 203\n",
      " 206 207 214 216 217 219 221 225 230 231 233 236 245 249 250 251 257 260\n",
      " 261 262 265 268 270 277 281 282 284 286 291 296 299 301 303 306 310 314\n",
      " 322 325 332 336 342 348 354 357 364 365 373 374 384 385 387 402 404 407\n",
      " 413 414 415 418 419 422 423 433 436]\n",
      "I am 9\n",
      "(9900, 128) (19800, 128)\n",
      "I am 10\n",
      "(9900, 19800)\n",
      "70: EER = 0.0602, FMR100 = 0.4700, Count = 200\n",
      "82: EER = 0.0047, FMR100 = 0.0000, Count = 200\n",
      "85: EER = 0.0434, FMR100 = 0.5700, Count = 200\n",
      "86: EER = 0.0010, FMR100 = 0.0000, Count = 200\n",
      "96: EER = 0.0610, FMR100 = 0.3400, Count = 200\n",
      "103: EER = 0.0042, FMR100 = 0.0000, Count = 200\n",
      "106: EER = 0.1007, FMR100 = 0.7200, Count = 200\n",
      "109: EER = 0.0237, FMR100 = 0.0600, Count = 200\n",
      "111: EER = 0.1308, FMR100 = 0.7900, Count = 200\n",
      "115: EER = 0.1849, FMR100 = 1.0000, Count = 200\n",
      "118: EER = 0.0238, FMR100 = 0.6600, Count = 200\n",
      "119: EER = 0.1219, FMR100 = 1.0000, Count = 200\n",
      "125: EER = 0.1800, FMR100 = 0.6600, Count = 200\n",
      "129: EER = 0.1700, FMR100 = 0.7300, Count = 200\n",
      "130: EER = 0.0003, FMR100 = 0.0000, Count = 200\n",
      "131: EER = 0.2940, FMR100 = 1.0000, Count = 200\n",
      "136: EER = 0.0413, FMR100 = 0.9500, Count = 200\n",
      "145: EER = 0.1936, FMR100 = 0.9900, Count = 200\n",
      "156: EER = 0.0518, FMR100 = 0.6000, Count = 200\n",
      "159: EER = 0.0500, FMR100 = 0.4400, Count = 200\n",
      "169: EER = 0.3536, FMR100 = 1.0000, Count = 200\n",
      "173: EER = 0.0439, FMR100 = 0.5800, Count = 200\n",
      "174: EER = 0.0429, FMR100 = 0.4800, Count = 200\n",
      "175: EER = 0.0210, FMR100 = 0.0400, Count = 200\n",
      "176: EER = 0.0030, FMR100 = 0.0000, Count = 200\n",
      "181: EER = 0.0614, FMR100 = 0.7000, Count = 200\n",
      "183: EER = 0.0216, FMR100 = 0.1600, Count = 200\n",
      "184: EER = 0.0200, FMR100 = 0.0900, Count = 200\n",
      "187: EER = 0.0400, FMR100 = 0.2900, Count = 200\n",
      "191: EER = 0.1000, FMR100 = 0.7500, Count = 200\n",
      "192: EER = 0.0417, FMR100 = 0.7700, Count = 200\n",
      "194: EER = 0.0511, FMR100 = 0.3900, Count = 200\n",
      "197: EER = 0.3315, FMR100 = 1.0000, Count = 200\n",
      "199: EER = 0.0200, FMR100 = 0.0500, Count = 200\n",
      "201: EER = 0.0527, FMR100 = 0.6000, Count = 200\n",
      "203: EER = 0.0216, FMR100 = 0.0600, Count = 200\n",
      "206: EER = 0.0220, FMR100 = 0.0800, Count = 200\n",
      "207: EER = 0.1443, FMR100 = 1.0000, Count = 200\n",
      "214: EER = 0.0113, FMR100 = 0.0200, Count = 200\n",
      "216: EER = 0.0212, FMR100 = 0.2300, Count = 200\n",
      "217: EER = 0.0015, FMR100 = 0.0000, Count = 200\n",
      "219: EER = 0.0147, FMR100 = 0.0500, Count = 200\n",
      "221: EER = 0.0342, FMR100 = 0.4600, Count = 200\n",
      "225: EER = 0.0800, FMR100 = 0.5800, Count = 200\n",
      "230: EER = 0.0619, FMR100 = 0.5200, Count = 200\n",
      "231: EER = 0.0541, FMR100 = 0.2600, Count = 200\n",
      "233: EER = 0.1100, FMR100 = 1.0000, Count = 200\n",
      "236: EER = 0.0714, FMR100 = 0.5300, Count = 200\n",
      "245: EER = 0.0400, FMR100 = 0.3600, Count = 200\n",
      "249: EER = 0.2300, FMR100 = 1.0000, Count = 200\n",
      "250: EER = 0.0329, FMR100 = 0.1400, Count = 200\n",
      "251: EER = 0.0300, FMR100 = 0.3300, Count = 200\n",
      "257: EER = 0.0300, FMR100 = 0.1900, Count = 200\n",
      "260: EER = 0.0313, FMR100 = 0.2500, Count = 200\n",
      "261: EER = 0.0936, FMR100 = 0.8000, Count = 200\n",
      "262: EER = 0.0043, FMR100 = 0.0000, Count = 200\n",
      "265: EER = 0.1500, FMR100 = 1.0000, Count = 200\n",
      "268: EER = 0.0939, FMR100 = 0.7700, Count = 200\n",
      "270: EER = 0.1200, FMR100 = 0.6400, Count = 200\n",
      "277: EER = 0.0329, FMR100 = 0.2100, Count = 200\n",
      "281: EER = 0.0301, FMR100 = 0.3100, Count = 200\n",
      "282: EER = 0.0402, FMR100 = 0.3600, Count = 200\n",
      "284: EER = 0.0300, FMR100 = 0.1800, Count = 200\n",
      "286: EER = 0.0700, FMR100 = 0.5300, Count = 200\n",
      "291: EER = 0.1948, FMR100 = 1.0000, Count = 200\n",
      "296: EER = 0.1230, FMR100 = 0.9700, Count = 200\n",
      "299: EER = 0.0336, FMR100 = 0.1100, Count = 200\n",
      "301: EER = 0.0303, FMR100 = 0.3100, Count = 200\n",
      "303: EER = 0.0100, FMR100 = 0.0100, Count = 200\n",
      "306: EER = 0.0341, FMR100 = 0.7000, Count = 200\n",
      "310: EER = 0.0504, FMR100 = 0.9700, Count = 200\n",
      "314: EER = 0.0605, FMR100 = 0.7300, Count = 200\n",
      "322: EER = 0.0400, FMR100 = 0.1000, Count = 200\n",
      "325: EER = 0.1135, FMR100 = 0.9400, Count = 200\n",
      "332: EER = 0.0005, FMR100 = 0.0000, Count = 200\n",
      "336: EER = 0.1320, FMR100 = 1.0000, Count = 200\n",
      "342: EER = 0.0500, FMR100 = 0.4300, Count = 200\n",
      "348: EER = 0.0600, FMR100 = 0.3600, Count = 200\n",
      "354: EER = 0.0306, FMR100 = 0.1000, Count = 200\n",
      "357: EER = 0.2016, FMR100 = 1.0000, Count = 200\n",
      "364: EER = 0.0226, FMR100 = 0.2000, Count = 200\n",
      "365: EER = 0.0300, FMR100 = 0.1300, Count = 200\n",
      "373: EER = 0.1048, FMR100 = 0.7400, Count = 200\n",
      "374: EER = 0.0218, FMR100 = 0.2500, Count = 200\n",
      "384: EER = 0.0200, FMR100 = 0.1700, Count = 200\n",
      "385: EER = 0.0300, FMR100 = 0.0700, Count = 200\n",
      "387: EER = 0.0027, FMR100 = 0.0000, Count = 200\n",
      "402: EER = 0.1521, FMR100 = 0.9700, Count = 200\n",
      "404: EER = 0.0702, FMR100 = 0.4900, Count = 200\n",
      "407: EER = 0.1516, FMR100 = 0.9800, Count = 200\n",
      "413: EER = 0.0648, FMR100 = 0.6900, Count = 200\n",
      "414: EER = 0.0242, FMR100 = 0.0700, Count = 200\n",
      "415: EER = 0.0603, FMR100 = 0.4400, Count = 200\n",
      "418: EER = 0.0913, FMR100 = 0.8900, Count = 200\n",
      "419: EER = 0.0400, FMR100 = 0.2500, Count = 200\n",
      "422: EER = 0.0208, FMR100 = 0.1100, Count = 200\n",
      "423: EER = 0.0500, FMR100 = 0.3600, Count = 200\n",
      "433: EER = 0.0019, FMR100 = 0.0000, Count = 200\n",
      "436: EER = 0.0200, FMR100 = 0.0600, Count = 200\n",
      "Final Average EER: 6.8657\n",
      "Final EER Standard Deviation: 6.9669\n",
      "$6.87 \\pm 6.97$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.865749330035044, 6.966859200985594)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#, s_test_b\n",
    "resutls2,resutls3=assessment_model_data_two2(x_test_e,y_test_e, x_test_v,y_test_v,None, s_test_v, distance = \"cd\")\n",
    "#print(\"simple\",EERf(resutls))\n",
    "calculate_and_print_averages(y_test_e, resutls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb69f01-12fe-40a6-8937-3e7832ef9b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
